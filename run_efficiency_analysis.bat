@echo off
set CUDA_VISIBLE_DEVICES=%1

:: Benchmark BERT
@REM python efficient_analysis.py --n_epochs 5 --train_batch_size %3 --model_name bert-base-uncased --step_size 1 --gamma 0.9 --experiment_name bert_benchmark_b%3_step1_gamma0.9_lr1e-4_early%2_layer2_lowerTrue --lr 1e-4 --early_stop %2 --dataset ag-news-normal --lower --num_layers 2 --subset_percentage 10 --force
@REM python efficient_analysis.py --n_epochs 5 --train_batch_size %3 --model_name bert-base-uncased --step_size 1 --gamma 0.9 --experiment_name bert_benchmark_b%3_step1_gamma0.9_lr1e-4_early%2_layer2_lowerTrue --lr 1e-4 --early_stop %2 --dataset ag-news-normal --lower --num_layers 2 --subset_percentage 20 --force
@REM python efficient_analysis.py --n_epochs 5 --train_batch_size %3 --model_name bert-base-uncased --step_size 1 --gamma 0.9 --experiment_name bert_benchmark_b%3_step1_gamma0.9_lr1e-4_early%2_layer2_lowerTrue --lr 1e-4 --early_stop %2 --dataset ag-news-normal --lower --num_layers 2 --subset_percentage 30 --force
@REM python efficient_analysis.py --n_epochs 5 --train_batch_size %3 --model_name bert-base-uncased --step_size 1 --gamma 0.9 --experiment_name bert_benchmark_b%3_step1_gamma0.9_lr1e-4_early%2_layer2_lowerTrue --lr 1e-4 --early_stop %2 --dataset ag-news-normal --lower --num_layers 2 --subset_percentage 40 --force
@REM python efficient_analysis.py --n_epochs 5 --train_batch_size %3 --model_name bert-base-uncased --step_size 1 --gamma 0.9 --experiment_name bert_benchmark_b%3_step1_gamma0.9_lr1e-4_early%2_layer2_lowerTrue --lr 1e-4 --early_stop %2 --dataset ag-news-normal --lower --num_layers 2 --subset_percentage 50 --force
@REM python efficient_analysis.py --n_epochs 5 --train_batch_size %3 --model_name bert-base-uncased --step_size 1 --gamma 0.9 --experiment_name bert_benchmark_b%3_step1_gamma0.9_lr1e-4_early%2_layer2_lowerTrue --lr 1e-4 --early_stop %2 --dataset ag-news-normal --lower --num_layers 2 --subset_percentage 60 --force
@REM python efficient_analysis.py --n_epochs 5 --train_batch_size %3 --model_name bert-base-uncased --step_size 1 --gamma 0.9 --experiment_name bert_benchmark_b%3_step1_gamma0.9_lr1e-4_early%2_layer2_lowerTrue --lr 1e-4 --early_stop %2 --dataset ag-news-normal --lower --num_layers 2 --subset_percentage 70 --force
@REM python efficient_analysis.py --n_epochs 5 --train_batch_size %3 --model_name bert-base-uncased --step_size 1 --gamma 0.9 --experiment_name bert_benchmark_b%3_step1_gamma0.9_lr1e-4_early%2_layer2_lowerTrue --lr 1e-4 --early_stop %2 --dataset ag-news-normal --lower --num_layers 2 --subset_percentage 80 --force
@REM python efficient_analysis.py --n_epochs 5 --train_batch_size %3 --model_name bert-base-uncased --step_size 1 --gamma 0.9 --experiment_name bert_benchmark_b%3_step1_gamma0.9_lr1e-4_early%2_layer2_lowerTrue --lr 1e-4 --early_stop %2 --dataset ag-news-normal --lower --num_layers 2 --subset_percentage 90 --force
@REM python efficient_analysis.py --n_epochs 5 --train_batch_size %3 --model_name bert-base-uncased --step_size 1 --gamma 0.9 --experiment_name bert_benchmark_b%3_step1_gamma0.9_lr1e-4_early%2_layer2_lowerTrue --lr 1e-4 --early_stop %2 --dataset ag-news-normal --lower --num_layers 2 --subset_percentage 100 --force

:: Benchmark ROBERTA
@REM python efficient_analysis.py --n_epochs 5 --train_batch_size %3 --model_name roberta-base --step_size 1 --gamma 0.9 --experiment_name roberta_benchmark_b%3_step1_gamma0.9_lr1e-4_early%2_layer2_lowerTrue --lr 1e-4 --early_stop %2 --dataset ag-news-normal --lower --num_layers 2 --subset_percentage 10 --force
@REM python efficient_analysis.py --n_epochs 5 --train_batch_size %3 --model_name roberta-base --step_size 1 --gamma 0.9 --experiment_name roberta_benchmark_b%3_step1_gamma0.9_lr1e-4_early%2_layer2_lowerTrue --lr 1e-4 --early_stop %2 --dataset ag-news-normal --lower --num_layers 2 --subset_percentage 20 --force
@REM python efficient_analysis.py --n_epochs 5 --train_batch_size %3 --model_name roberta-base --step_size 1 --gamma 0.9 --experiment_name roberta_benchmark_b%3_step1_gamma0.9_lr1e-4_early%2_layer2_lowerTrue --lr 1e-4 --early_stop %2 --dataset ag-news-normal --lower --num_layers 2 --subset_percentage 30 --force
@REM python efficient_analysis.py --n_epochs 5 --train_batch_size %3 --model_name roberta-base --step_size 1 --gamma 0.9 --experiment_name roberta_benchmark_b%3_step1_gamma0.9_lr1e-4_early%2_layer2_lowerTrue --lr 1e-4 --early_stop %2 --dataset ag-news-normal --lower --num_layers 2 --subset_percentage 40 --force
@REM python efficient_analysis.py --n_epochs 5 --train_batch_size %3 --model_name roberta-base --step_size 1 --gamma 0.9 --experiment_name roberta_benchmark_b%3_step1_gamma0.9_lr1e-4_early%2_layer2_lowerTrue --lr 1e-4 --early_stop %2 --dataset ag-news-normal --lower --num_layers 2 --subset_percentage 50 --force
@REM python efficient_analysis.py --n_epochs 5 --train_batch_size %3 --model_name roberta-base --step_size 1 --gamma 0.9 --experiment_name roberta_benchmark_b%3_step1_gamma0.9_lr1e-4_early%2_layer2_lowerTrue --lr 1e-4 --early_stop %2 --dataset ag-news-normal --lower --num_layers 2 --subset_percentage 60 --force
@REM python efficient_analysis.py --n_epochs 5 --train_batch_size %3 --model_name roberta-base --step_size 1 --gamma 0.9 --experiment_name roberta_benchmark_b%3_step1_gamma0.9_lr1e-4_early%2_layer2_lowerTrue --lr 1e-4 --early_stop %2 --dataset ag-news-normal --lower --num_layers 2 --subset_percentage 70 --force
@REM python efficient_analysis.py --n_epochs 5 --train_batch_size %3 --model_name roberta-base --step_size 1 --gamma 0.9 --experiment_name roberta_benchmark_b%3_step1_gamma0.9_lr1e-4_early%2_layer2_lowerTrue --lr 1e-4 --early_stop %2 --dataset ag-news-normal --lower --num_layers 2 --subset_percentage 80 --force
@REM python efficient_analysis.py --n_epochs 5 --train_batch_size %3 --model_name roberta-base --step_size 1 --gamma 0.9 --experiment_name roberta_benchmark_b%3_step1_gamma0.9_lr1e-4_early%2_layer2_lowerTrue --lr 1e-4 --early_stop %2 --dataset ag-news-normal --lower --num_layers 2 --subset_percentage 90 --force
@REM python efficient_analysis.py --n_epochs 5 --train_batch_size %3 --model_name roberta-base --step_size 1 --gamma 0.9 --experiment_name roberta_benchmark_b%3_step1_gamma0.9_lr1e-4_early%2_layer2_lowerTrue --lr 1e-4 --early_stop %2 --dataset ag-news-normal --lower --num_layers 2 --subset_percentage 100 --force

:: Modified BERT SVD
@REM python efficient_analysis.py --n_epochs 5 --train_batch_size %3 --model_name bilstm-dim-reduction --step_size 1 --gamma 0.9 --seed 88 --experiment_name ag-news-bert-whitening-SVD --lr 1e-3 --eps 1e-8 --early_stop %2 --dataset ag-news-bert-whitening --lower --num_layers 2 --subset_percentage 10 --force
@REM python efficient_analysis.py --n_epochs 5 --train_batch_size %3 --model_name bilstm-dim-reduction --step_size 1 --gamma 0.9 --seed 88 --experiment_name ag-news-bert-whitening-SVD --lr 1e-3 --eps 1e-8 --early_stop %2 --dataset ag-news-bert-whitening --lower --num_layers 2 --subset_percentage 20 --force
@REM python efficient_analysis.py --n_epochs 5 --train_batch_size %3 --model_name bilstm-dim-reduction --step_size 1 --gamma 0.9 --seed 88 --experiment_name ag-news-bert-whitening-SVD --lr 1e-3 --eps 1e-8 --early_stop %2 --dataset ag-news-bert-whitening --lower --num_layers 2 --subset_percentage 30 --force
@REM python efficient_analysis.py --n_epochs 5 --train_batch_size %3 --model_name bilstm-dim-reduction --step_size 1 --gamma 0.9 --seed 88 --experiment_name ag-news-bert-whitening-SVD --lr 1e-3 --eps 1e-8 --early_stop %2 --dataset ag-news-bert-whitening --lower --num_layers 2 --subset_percentage 40 --force
@REM python efficient_analysis.py --n_epochs 5 --train_batch_size %3 --model_name bilstm-dim-reduction --step_size 1 --gamma 0.9 --seed 88 --experiment_name ag-news-bert-whitening-SVD --lr 1e-3 --eps 1e-8 --early_stop %2 --dataset ag-news-bert-whitening --lower --num_layers 2 --subset_percentage 50 --force
@REM python efficient_analysis.py --n_epochs 5 --train_batch_size %3 --model_name bilstm-dim-reduction --step_size 1 --gamma 0.9 --seed 88 --experiment_name ag-news-bert-whitening-SVD --lr 1e-3 --eps 1e-8 --early_stop %2 --dataset ag-news-bert-whitening --lower --num_layers 2 --subset_percentage 60 --force
@REM python efficient_analysis.py --n_epochs 5 --train_batch_size %3 --model_name bilstm-dim-reduction --step_size 1 --gamma 0.9 --seed 88 --experiment_name ag-news-bert-whitening-SVD --lr 1e-3 --eps 1e-8 --early_stop %2 --dataset ag-news-bert-whitening --lower --num_layers 2 --subset_percentage 70 --force
@REM python efficient_analysis.py --n_epochs 5 --train_batch_size %3 --model_name bilstm-dim-reduction --step_size 1 --gamma 0.9 --seed 88 --experiment_name ag-news-bert-whitening-SVD --lr 1e-3 --eps 1e-8 --early_stop %2 --dataset ag-news-bert-whitening --lower --num_layers 2 --subset_percentage 80 --force
@REM python efficient_analysis.py --n_epochs 5 --train_batch_size %3 --model_name bilstm-dim-reduction --step_size 1 --gamma 0.9 --seed 88 --experiment_name ag-news-bert-whitening-SVD --lr 1e-3 --eps 1e-8 --early_stop %2 --dataset ag-news-bert-whitening --lower --num_layers 2 --subset_percentage 90 --force
@REM python efficient_analysis.py --n_epochs 5 --train_batch_size %3 --model_name bilstm-dim-reduction --step_size 1 --gamma 0.9 --seed 88 --experiment_name ag-news-bert-whitening-SVD --lr 1e-3 --eps 1e-8 --early_stop %2 --dataset ag-news-bert-whitening --lower --num_layers 2 --subset_percentage 100 --force

:: Modified ROBERTA SVD
@REM python efficient_analysis.py --n_epochs 5 --train_batch_size %3 --model_name bilstm-dim-reduction --step_size 1 --gamma 0.9 --seed 88 --experiment_name ag-news-roberta-whitening-SVD --lr 1e-3 --eps 1e-8 --early_stop %2 --dataset ag-news-roberta-whitening --lower --num_layers 2 --subset_percentage 10 --force
@REM python efficient_analysis.py --n_epochs 5 --train_batch_size %3 --model_name bilstm-dim-reduction --step_size 1 --gamma 0.9 --seed 88 --experiment_name ag-news-roberta-whitening-SVD --lr 1e-3 --eps 1e-8 --early_stop %2 --dataset ag-news-roberta-whitening --lower --num_layers 2 --subset_percentage 20 --force
@REM python efficient_analysis.py --n_epochs 5 --train_batch_size %3 --model_name bilstm-dim-reduction --step_size 1 --gamma 0.9 --seed 88 --experiment_name ag-news-roberta-whitening-SVD --lr 1e-3 --eps 1e-8 --early_stop %2 --dataset ag-news-roberta-whitening --lower --num_layers 2 --subset_percentage 30 --force
@REM python efficient_analysis.py --n_epochs 5 --train_batch_size %3 --model_name bilstm-dim-reduction --step_size 1 --gamma 0.9 --seed 88 --experiment_name ag-news-roberta-whitening-SVD --lr 1e-3 --eps 1e-8 --early_stop %2 --dataset ag-news-roberta-whitening --lower --num_layers 2 --subset_percentage 40 --force
@REM python efficient_analysis.py --n_epochs 5 --train_batch_size %3 --model_name bilstm-dim-reduction --step_size 1 --gamma 0.9 --seed 88 --experiment_name ag-news-roberta-whitening-SVD --lr 1e-3 --eps 1e-8 --early_stop %2 --dataset ag-news-roberta-whitening --lower --num_layers 2 --subset_percentage 50 --force
@REM python efficient_analysis.py --n_epochs 5 --train_batch_size %3 --model_name bilstm-dim-reduction --step_size 1 --gamma 0.9 --seed 88 --experiment_name ag-news-roberta-whitening-SVD --lr 1e-3 --eps 1e-8 --early_stop %2 --dataset ag-news-roberta-whitening --lower --num_layers 2 --subset_percentage 60 --force
@REM python efficient_analysis.py --n_epochs 5 --train_batch_size %3 --model_name bilstm-dim-reduction --step_size 1 --gamma 0.9 --seed 88 --experiment_name ag-news-roberta-whitening-SVD --lr 1e-3 --eps 1e-8 --early_stop %2 --dataset ag-news-roberta-whitening --lower --num_layers 2 --subset_percentage 70 --force
@REM python efficient_analysis.py --n_epochs 5 --train_batch_size %3 --model_name bilstm-dim-reduction --step_size 1 --gamma 0.9 --seed 88 --experiment_name ag-news-roberta-whitening-SVD --lr 1e-3 --eps 1e-8 --early_stop %2 --dataset ag-news-roberta-whitening --lower --num_layers 2 --subset_percentage 80 --force
@REM python efficient_analysis.py --n_epochs 5 --train_batch_size %3 --model_name bilstm-dim-reduction --step_size 1 --gamma 0.9 --seed 88 --experiment_name ag-news-roberta-whitening-SVD --lr 1e-3 --eps 1e-8 --early_stop %2 --dataset ag-news-roberta-whitening --lower --num_layers 2 --subset_percentage 90 --force
@REM python efficient_analysis.py --n_epochs 5 --train_batch_size %3 --model_name bilstm-dim-reduction --step_size 1 --gamma 0.9 --seed 88 --experiment_name ag-news-roberta-whitening-SVD --lr 1e-3 --eps 1e-8 --early_stop %2 --dataset ag-news-roberta-whitening --lower --num_layers 2 --subset_percentage 100 --force

:: Modified BERT ZCA
python efficient_analysis.py --n_epochs 5 --train_batch_size %3 --model_name bilstm-dim-reduction --step_size 1 --gamma 0.9 --seed 88 --experiment_name ag-news-bert-whitening-zca-modified --lr 1.8e-3 --eps 1e-8 --early_stop %2 --dataset ag-news-bert-whitening --lower --num_layers 2 --subset_percentage 10 --force
python efficient_analysis.py --n_epochs 5 --train_batch_size %3 --model_name bilstm-dim-reduction --step_size 1 --gamma 0.9 --seed 88 --experiment_name ag-news-bert-whitening-zca-modified --lr 1.8e-3 --eps 1e-8 --early_stop %2 --dataset ag-news-bert-whitening --lower --num_layers 2 --subset_percentage 20 --force
python efficient_analysis.py --n_epochs 5 --train_batch_size %3 --model_name bilstm-dim-reduction --step_size 1 --gamma 0.9 --seed 88 --experiment_name ag-news-bert-whitening-zca-modified --lr 1.8e-3 --eps 1e-8 --early_stop %2 --dataset ag-news-bert-whitening --lower --num_layers 2 --subset_percentage 30 --force
python efficient_analysis.py --n_epochs 5 --train_batch_size %3 --model_name bilstm-dim-reduction --step_size 1 --gamma 0.9 --seed 88 --experiment_name ag-news-bert-whitening-zca-modified --lr 1.8e-3 --eps 1e-8 --early_stop %2 --dataset ag-news-bert-whitening --lower --num_layers 2 --subset_percentage 40 --force
python efficient_analysis.py --n_epochs 5 --train_batch_size %3 --model_name bilstm-dim-reduction --step_size 1 --gamma 0.9 --seed 88 --experiment_name ag-news-bert-whitening-zca-modified --lr 1.8e-3 --eps 1e-8 --early_stop %2 --dataset ag-news-bert-whitening --lower --num_layers 2 --subset_percentage 50 --force
python efficient_analysis.py --n_epochs 5 --train_batch_size %3 --model_name bilstm-dim-reduction --step_size 1 --gamma 0.9 --seed 88 --experiment_name ag-news-bert-whitening-zca-modified --lr 1.8e-3 --eps 1e-8 --early_stop %2 --dataset ag-news-bert-whitening --lower --num_layers 2 --subset_percentage 60 --force
python efficient_analysis.py --n_epochs 5 --train_batch_size %3 --model_name bilstm-dim-reduction --step_size 1 --gamma 0.9 --seed 88 --experiment_name ag-news-bert-whitening-zca-modified --lr 1.8e-3 --eps 1e-8 --early_stop %2 --dataset ag-news-bert-whitening --lower --num_layers 2 --subset_percentage 70 --force
python efficient_analysis.py --n_epochs 5 --train_batch_size %3 --model_name bilstm-dim-reduction --step_size 1 --gamma 0.9 --seed 88 --experiment_name ag-news-bert-whitening-zca-modified --lr 1.8e-3 --eps 1e-8 --early_stop %2 --dataset ag-news-bert-whitening --lower --num_layers 2 --subset_percentage 80 --force
python efficient_analysis.py --n_epochs 5 --train_batch_size %3 --model_name bilstm-dim-reduction --step_size 1 --gamma 0.9 --seed 88 --experiment_name ag-news-bert-whitening-zca-modified --lr 1.8e-3 --eps 1e-8 --early_stop %2 --dataset ag-news-bert-whitening --lower --num_layers 2 --subset_percentage 90 --force
python efficient_analysis.py --n_epochs 5 --train_batch_size %3 --model_name bilstm-dim-reduction --step_size 1 --gamma 0.9 --seed 88 --experiment_name ag-news-bert-whitening-zca-modified --lr 1.8e-3 --eps 1e-8 --early_stop %2 --dataset ag-news-bert-whitening --lower --num_layers 2 --subset_percentage 100 --force

:: Modified ROBERTA ZCA
python efficient_analysis.py --n_epochs 5 --train_batch_size %3 --model_name bilstm-dim-reduction --step_size 1 --gamma 0.9 --seed 42 --experiment_name ag-news-roberta-whitening-zca-modified --lr 1.8e-3 --eps 1e-8 --early_stop %2 --dataset ag-news-roberta-whitening --lower --num_layers 2 --subset_percentage 10 --force
python efficient_analysis.py --n_epochs 5 --train_batch_size %3 --model_name bilstm-dim-reduction --step_size 1 --gamma 0.9 --seed 42 --experiment_name ag-news-roberta-whitening-zca-modified --lr 1.8e-3 --eps 1e-8 --early_stop %2 --dataset ag-news-roberta-whitening --lower --num_layers 2 --subset_percentage 20 --force
python efficient_analysis.py --n_epochs 5 --train_batch_size %3 --model_name bilstm-dim-reduction --step_size 1 --gamma 0.9 --seed 42 --experiment_name ag-news-roberta-whitening-zca-modified --lr 1.8e-3 --eps 1e-8 --early_stop %2 --dataset ag-news-roberta-whitening --lower --num_layers 2 --subset_percentage 30 --force
python efficient_analysis.py --n_epochs 5 --train_batch_size %3 --model_name bilstm-dim-reduction --step_size 1 --gamma 0.9 --seed 42 --experiment_name ag-news-roberta-whitening-zca-modified --lr 1.8e-3 --eps 1e-8 --early_stop %2 --dataset ag-news-roberta-whitening --lower --num_layers 2 --subset_percentage 40 --force
python efficient_analysis.py --n_epochs 5 --train_batch_size %3 --model_name bilstm-dim-reduction --step_size 1 --gamma 0.9 --seed 42 --experiment_name ag-news-roberta-whitening-zca-modified --lr 1.8e-3 --eps 1e-8 --early_stop %2 --dataset ag-news-roberta-whitening --lower --num_layers 2 --subset_percentage 50 --force
python efficient_analysis.py --n_epochs 5 --train_batch_size %3 --model_name bilstm-dim-reduction --step_size 1 --gamma 0.9 --seed 42 --experiment_name ag-news-roberta-whitening-zca-modified --lr 1.8e-3 --eps 1e-8 --early_stop %2 --dataset ag-news-roberta-whitening --lower --num_layers 2 --subset_percentage 60 --force
python efficient_analysis.py --n_epochs 5 --train_batch_size %3 --model_name bilstm-dim-reduction --step_size 1 --gamma 0.9 --seed 42 --experiment_name ag-news-roberta-whitening-zca-modified --lr 1.8e-3 --eps 1e-8 --early_stop %2 --dataset ag-news-roberta-whitening --lower --num_layers 2 --subset_percentage 70 --force
python efficient_analysis.py --n_epochs 5 --train_batch_size %3 --model_name bilstm-dim-reduction --step_size 1 --gamma 0.9 --seed 42 --experiment_name ag-news-roberta-whitening-zca-modified --lr 1.8e-3 --eps 1e-8 --early_stop %2 --dataset ag-news-roberta-whitening --lower --num_layers 2 --subset_percentage 80 --force
python efficient_analysis.py --n_epochs 5 --train_batch_size %3 --model_name bilstm-dim-reduction --step_size 1 --gamma 0.9 --seed 42 --experiment_name ag-news-roberta-whitening-zca-modified --lr 1.8e-3 --eps 1e-8 --early_stop %2 --dataset ag-news-roberta-whitening --lower --num_layers 2 --subset_percentage 90 --force
python efficient_analysis.py --n_epochs 5 --train_batch_size %3 --model_name bilstm-dim-reduction --step_size 1 --gamma 0.9 --seed 42 --experiment_name ag-news-roberta-whitening-zca-modified --lr 1.8e-3 --eps 1e-8 --early_stop %2 --dataset ag-news-roberta-whitening --lower --num_layers 2 --subset_percentage 100 --force
