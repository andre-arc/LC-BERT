{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\Andri\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\Andri\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package omw-1.4 to\n",
      "[nltk_data]     C:\\Users\\Andri\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package omw-1.4 is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "import re\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from utils.preprocessing import clean\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.decomposition import PCA\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def set_seed(seed):\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed(seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BertWhiteningDataset(Dataset):\n",
    "    LABEL2INDEX = {'world': 0, 'sports': 1, 'business': 2, 'science': 3}\n",
    "    INDEX2LABEL = {0: 'world', 1: 'sports', 2: 'business', 3: 'science'}\n",
    "    NUM_LABELS = 4\n",
    "\n",
    "    def __init__(self, device, dataset_path, tokenizer, model, max_len, *args, **kwargs):\n",
    "        self.device = device\n",
    "        self.data = self.load_dataset(dataset_path)\n",
    "        self.tokenizer = tokenizer\n",
    "        self.model = model.to(self.device)\n",
    "        # self.vecs_after, self.vecs_before = self.Dim_reduction(max_len=max_len)\n",
    "        self.embedding = self.get_embeddings(max_len=max_len)\n",
    "        \n",
    "    def __getitem__(self, index):\n",
    "        data = self.data.loc[index,:]\n",
    "        # vecs_before = torch.from_numpy(self.vecs_before[index]).unsqueeze(0).float()\n",
    "        embedding = torch.from_numpy(self.embedding[index]).unsqueeze(0).float()\n",
    "        vecs, seq_label = embedding, data['target']\n",
    "        \n",
    "        return vecs, np.array(seq_label), data['cleaned_text']\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def load_dataset(self, path):\n",
    "        dataset = []\n",
    "        # Read file\n",
    "        data = pd.read_csv(path)\n",
    "\n",
    "        # label encoder\n",
    "        class_names = set(data.iloc[:,0].values)\n",
    "        le = LabelEncoder()\n",
    "        le.fit(data.iloc[:,0])\n",
    "        class_names = le.classes_\n",
    "\n",
    "        data['target'] = le.transform(data.iloc[:,0])\n",
    "\n",
    "        #clean docs\n",
    "        data['cleaned_text'] = clean(data['Description'])\n",
    "\n",
    "        return data\n",
    "    \n",
    "    def get_embeddings(self, max_len):\n",
    "        \n",
    "        if(self.tokenizer is None or self.model is None):\n",
    "            raise Exception(\"Sorry, But You must define Tokenizer and Model\")\n",
    "    \n",
    "        vecs = []\n",
    "        with torch.no_grad():\n",
    "          i = 1\n",
    "          for sentence in self.data['cleaned_text']:\n",
    "            inputs = self.tokenizer.encode_plus(sentence, return_tensors=\"pt\",max_length=max_len, return_attention_mask=True,truncation=True)\n",
    "            inputs['input_ids'] = inputs['input_ids'].to(self.device)\n",
    "            inputs['attention_mask'] = inputs['attention_mask'].to(self.device)\n",
    "\n",
    "            hidden_states = self.model(input_ids=inputs['input_ids'], attention_mask=inputs['attention_mask'], return_dict=True, output_hidden_states=True).hidden_states\n",
    "\n",
    "            #Averaging the first & last hidden states\n",
    "            output_hidden_state = (hidden_states[-1] + hidden_states[1]).mean(dim=1)\n",
    "\n",
    "            vec = output_hidden_state.cpu().numpy()[0]\n",
    "\n",
    "            print(f'\\rWord Embedding Process: {i} / {len(self.data[\"cleaned_text\"])} words | GPU Usages: {(torch.cuda.memory_allocated() / 1048576):3.1f}', end=' ')\n",
    "            i+=1\n",
    "\n",
    "            # print(vec.shape)\n",
    "            vecs.append(vec)\n",
    "    \n",
    "        return np.array(vecs)\n",
    "\n",
    "    def _transform_and_normalize(self, vecs, kernel, bias):\n",
    "        \"\"\"\n",
    "            Applying transformation then standardize\n",
    "        \"\"\"\n",
    "        if not (kernel is None or bias is None):\n",
    "            vecs = (vecs + bias).dot(kernel)\n",
    "        return self._normalize(vecs)\n",
    "        \n",
    "    def _normalize(self, vecs):\n",
    "        \"\"\"\n",
    "            Standardization\n",
    "        \"\"\"\n",
    "        return vecs / (vecs**2).sum(axis=1, keepdims=True)**0.5\n",
    "        \n",
    "    def _compute_kernel_bias_svd(self, vecs):\n",
    "        \"\"\"\n",
    "        Calculate Kernal & Bias for the final transformation - y = (x + bias).dot(kernel)\n",
    "        \"\"\"\n",
    "        vecs = np.concatenate(vecs, axis=0)\n",
    "        mu = vecs.mean(axis=0, keepdims=True)\n",
    "        cov = np.cov(vecs.T)\n",
    "        u, s, vh = np.linalg.svd(cov)\n",
    "        W = np.dot(u, np.diag(s**0.5))\n",
    "        W = np.linalg.inv(W.T)\n",
    "        return W, -mu\n",
    "\n",
    "    def _compute_kernel_bias_pca(self, vecs):\n",
    "        vecs = np.concatenate(vecs, axis=0)\n",
    "        mu = vecs.mean(axis=0, keepdims=True)\n",
    "        \n",
    "        # Apply PCA\n",
    "        pca = PCA()\n",
    "        pca.fit(vecs)\n",
    "\n",
    "        # Obtain the transformation matrix W\n",
    "        W = pca.components_.T\n",
    "\n",
    "        # Invert and transpose W\n",
    "        # W = np.linalg.inv(W.T)\n",
    "        return W, -mu\n",
    "    \n",
    "    def _compute_kernel_bias_zca(self, vecs):\n",
    "        vecs = np.concatenate(vecs, axis=0)\n",
    "        mu = vecs.mean(axis=0, keepdims=True)\n",
    "        \n",
    "        # Center the data\n",
    "        centered_vecs = vecs - vecs.mean(axis=0, keepdims=True)\n",
    "\n",
    "        # Apply PCA\n",
    "        pca = PCA()\n",
    "        pca.fit(centered_vecs)\n",
    "\n",
    "        # Calculate eigenvalues and eigenvectors\n",
    "        eigenvalues, eigenvectors = np.linalg.eig(pca.components_)\n",
    "\n",
    "        # Apply ZCA whitening\n",
    "        zca_matrix = eigenvectors * np.diag(1 / np.sqrt(eigenvalues + 1e-5)) @ eigenvectors.T\n",
    "\n",
    "        # Return ZCA whitening matrix and mean\n",
    "        return zca_matrix, -mu\n",
    "\n",
    "    def _compute_kernel_bias_sphere(self, vecs):\n",
    "        vecs = np.concatenate(vecs, axis=0)\n",
    "        mu = vecs.mean(axis=0, keepdims=True)\n",
    "\n",
    "        # Normalize data length\n",
    "        normalized_vecs = vecs / np.linalg.norm(vecs, axis=1)[:, np.newaxis]\n",
    "\n",
    "        # Return whitening matrix (identity matrix in this case) and mean\n",
    "        return np.eye(normalized_vecs.shape[1]), -mu\n",
    "\n",
    "    def save_array(self, array, filename):\n",
    "        vecs = np.array(array)\n",
    "        vecs.save(f'{filename}.npy', vecs)\n",
    "\n",
    "    #Defining a function to extract feature using Bert and Dimensionality Reduction\n",
    "    def Dim_reduction(self, dim_reduction_func='whitening', target_dim=256):\n",
    "        '''\n",
    "            This method will accept array of sentences, roberta tokenizer & model\n",
    "            next it will call methods for dimention reduction\n",
    "        '''\n",
    "        \n",
    "        torch.cuda.empty_cache()\n",
    "\n",
    "        vecs = self.embedding\n",
    "\n",
    "        match dim_reduction_func:\n",
    "            case \"pca\":\n",
    "                kernel, bias = self._compute_kernel_bias_pca([vecs])\n",
    "            case \"zca\":\n",
    "                kernel, bias = self._compute_kernel_bias_zca([vecs])\n",
    "            case \"sphere\":\n",
    "                kernel, bias = self._compute_kernel_bias_sphere([vecs])\n",
    "            case _:\n",
    "                kernel, bias = self._compute_kernel_bias_svd([vecs])\n",
    "\n",
    "        #Finding Kernal\n",
    "        # kernel, bias = self._compute_kernel_bias_svd([vecs])\n",
    "        # kernel, bias = self._compute_kernel_bias_sphere([vecs])\n",
    "        kernel = kernel[:, :target_dim]\n",
    "        \n",
    "        #If you want to reduce it to 128 dim\n",
    "        #kernel = kernel[:, :128]\n",
    "        embeddings = []\n",
    "        embeddings = np.vstack(vecs)\n",
    "    \n",
    "        #Sentence embeddings can be converted into an identity matrix\n",
    "        #by utilizing the transformation matrix\n",
    "        embeddings = self._transform_and_normalize(embeddings, \n",
    "                    kernel=kernel,\n",
    "                    bias=bias\n",
    "                )\n",
    "    \n",
    "        return embeddings\n",
    "\n",
    "class BertWhiteningDataLoader(DataLoader):\n",
    "    def __init__(self, dataset, batch_size=1, shuffle=False, num_workers=0, **kwargs):\n",
    "        super(BertWhiteningDataLoader, self).__init__(\n",
    "            dataset,\n",
    "            batch_size=batch_size,\n",
    "            shuffle=shuffle,\n",
    "            num_workers=num_workers,\n",
    "            **kwargs\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import BertConfig, BertTokenizer, RobertaConfig, RobertaTokenizer, BertModel, RobertaModel\n",
    "from modules.word_classification import BertForWordClassification, RobertaForWordClassification\n",
    "from modules.modified_word_classification import BiLSTMForWordClassification\n",
    "from sklearn.metrics import silhouette_score, davies_bouldin_score, mean_squared_error\n",
    "\n",
    "def load_extraction_model(model_name):\n",
    "    if 'bert-base-uncased' in model_name:\n",
    "        # bert-base-multilingual-uncased or bert-base-multilingual-cased\n",
    "        # Prepare config & tokenizer\n",
    "        vocab_path, config_path = None, None\n",
    "        tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
    "        config = BertConfig.from_pretrained('bert-base-uncased')\n",
    "        \n",
    "        # Instantiate model\n",
    "        model = BertModel.from_pretrained('bert-base-uncased', config=config)\n",
    "\n",
    "    elif 'roberta-base' in model_name:\n",
    "\n",
    "        # Prepare config & tokenizer\n",
    "        vocab_path, config_path = None, None\n",
    "        tokenizer = RobertaTokenizer.from_pretrained('roberta-base')            \n",
    "        config = RobertaConfig.from_pretrained('roberta-base')\n",
    "\n",
    "        # Instantiate model\n",
    "        model = RobertaModel.from_pretrained('roberta-base', config=config)\n",
    "\n",
    "    return model, tokenizer, vocab_path, config_path\n",
    "\n",
    "def load_metrics_function(metric_name, parameters):\n",
    "    match metric_name:\n",
    "        case \"silhouette_score\":\n",
    "            score = silhouette_score(parameters[0], parameters[1])\n",
    "        case \"davies_bouldin_score\":\n",
    "            score = davies_bouldin_score(parameters[0], parameters[1])\n",
    "        case \"RSME\":\n",
    "            score = mean_squared_error(parameters[0], parameters[1], squared=False)\n",
    "        case _:\n",
    "            score = mean_squared_error(parameters[0], parameters[1])\n",
    "    return score\n",
    "        \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "config = {\n",
    "    'seed': 88,\n",
    "    'max_seq_len': 512,\n",
    "    'target_dim': 256,\n",
    "    'device': 'cuda',\n",
    "    'train_set_path': './dataset/ag-news/train.csv',\n",
    "    'valid_set_path': './dataset/ag-news/valid.csv',\n",
    "    'test_set_path': './dataset/ag-news/test.csv',\n",
    "    'lower': True,\n",
    "    'no_special_token': True\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.predictions.transform.LayerNorm.weight', 'cls.seq_relationship.weight', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.decoder.weight', 'cls.predictions.transform.dense.bias', 'cls.seq_relationship.bias', 'cls.predictions.bias']\n",
      "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of the model checkpoint at roberta-base were not used when initializing RobertaModel: ['lm_head.layer_norm.bias', 'lm_head.decoder.weight', 'lm_head.dense.bias', 'lm_head.bias', 'lm_head.layer_norm.weight', 'lm_head.dense.weight']\n",
      "- This IS expected if you are initializing RobertaModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing RobertaModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    }
   ],
   "source": [
    "bertExtractionModel, bertExtractionTokenizer, a, b_ = load_extraction_model('bert-base-uncased')\n",
    "robertaExtractionModel, robertaExtractionTokenizer, c, d_ = load_extraction_model('roberta-base')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Word Embedding Process: 96000 / 96000 words | GPU Usages: 896.2                "
     ]
    }
   ],
   "source": [
    "train_dataset_path = config['train_set_path']\n",
    "bert_train_dataset = BertWhiteningDataset(config['device'], train_dataset_path, bertExtractionTokenizer, bertExtractionModel, config['max_seq_len'],  lowercase=config[\"lower\"], no_special_token=config['no_special_token'])\n",
    "roberta_train_dataset = BertWhiteningDataset(config['device'], train_dataset_path, robertaExtractionTokenizer, robertaExtractionModel, config['max_seq_len'],  lowercase=config[\"lower\"], no_special_token=config['no_special_token'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[-0.02727088-0.22772023j, -0.02459634-0.11850352j,\n",
       "        -0.30723065-0.40602404j, ...,  0.17181909-0.18204455j,\n",
       "        -0.22385319-0.12064982j,  0.06281056+0.04826992j],\n",
       "       [-0.04342636+0.08429575j, -0.0378532 -0.02161417j,\n",
       "        -0.14441566-0.10563593j, ..., -0.0595291 +0.27124202j,\n",
       "        -0.02837197-0.0368642j ,  0.07807039-0.2551869j ],\n",
       "       [ 0.11020361+0.22026157j,  0.18165302+0.10318858j,\n",
       "         0.17027366+0.05478154j, ..., -0.08908857-0.03835643j,\n",
       "        -0.05969416+0.0337906j ,  0.02623323+0.0381743j ],\n",
       "       ...,\n",
       "       [-0.15509884-0.03495263j,  0.04474313+0.14986989j,\n",
       "         0.08338162+0.13704787j, ..., -0.0112048 +0.05028997j,\n",
       "         0.18039005-0.1728627j , -0.18168011+0.04319514j],\n",
       "       [ 0.32423055-0.14345236j,  0.07797093-0.12513405j,\n",
       "        -0.05214187+0.10324172j, ...,  0.27159312+0.11581834j,\n",
       "        -0.02615654+0.09628966j,  0.00185102-0.01027462j],\n",
       "       [-0.12510677+0.00819267j, -0.1937687 -0.1704889j ,\n",
       "        -0.01736809+0.1406676j , ...,  0.01633977+0.19682315j,\n",
       "         0.06416851-0.10191231j, -0.05810755-0.03626566j]],\n",
       "      dtype=complex64)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "zca = bert_train_dataset.Dim_reduction('zca')\n",
    "zca"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[False False False ... False False False]\n",
      " [False False False ... False False False]\n",
      " [False False False ... False False False]\n",
      " ...\n",
      " [False False False ... False False False]\n",
      " [False False False ... False False False]\n",
      " [False False False ... False False False]]\n",
      "[False False  True  True  True  True  True  True  True  True  True  True\n",
      "  True  True  True  True  True  True False  True  True False  True  True\n",
      "  True  True  True  True  True  True  True  True  True  True  True  True\n",
      " False  True  True  True  True  True  True False False False False False\n",
      " False False]\n"
     ]
    }
   ],
   "source": [
    "vecs = np.random.rand(96000, 768)\n",
    "\n",
    "vecs = np.concatenate([vecs], axis=0)\n",
    "mu = vecs.mean(axis=0, keepdims=True)\n",
    "\n",
    "# Center the data\n",
    "centered_vecs = vecs - vecs.mean(axis=0, keepdims=True)\n",
    "\n",
    "# Apply PCA\n",
    "pca = PCA()\n",
    "pca.fit(centered_vecs)\n",
    "\n",
    "eigenvalues, eigenvectors = np.linalg.eig(np.random.rand(50, 50))\n",
    "print(np.iscomplex(pca.components_))\n",
    "print(np.iscomplex(eigenvalues))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "methods = [\"whitening\", \"pca\", \"sphere\"]\n",
    "metrics = [\"silhouette_score\", \"davies_bouldin_score\", \"MSE\", \"RMSE\"]\n",
    "architectures = [\"BERT\", \"ROBERTA\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = {\n",
    "    \"BERT\": {\n",
    "        \"silhouette_score\": [],\n",
    "        \"davies_bouldin_score\": [],\n",
    "        \"MSE\": [],\n",
    "        \"RMSE\": []\n",
    "    },\n",
    "    \"ROBERTA\": {\n",
    "        \"silhouette_score\": [],\n",
    "        \"davies_bouldin_score\": [],\n",
    "        \"MSE\": [],\n",
    "        \"RMSE\": []\n",
    "    }\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def complex_manhattan_distance(z1, z2):\n",
    "  \"\"\"\n",
    "  Calculates the complex Manhattan distance between two complex numbers.\n",
    "\n",
    "  Args:\n",
    "      z1: The first complex number (a+bi).\n",
    "      z2: The second complex number (c+di).\n",
    "\n",
    "  Returns:\n",
    "      The complex Manhattan distance between z1 and z2.\n",
    "  \"\"\"\n",
    "  return np.abs(np.real(z1) - np.real(z2)) + np.abs(np.imag(z1) - np.imag(z2))\n",
    "\n",
    "\n",
    "def adjusted_silhouette_score(X, labels):\n",
    "  \"\"\"\n",
    "  Calculates the Silhouette score using the complex Manhattan distance, \n",
    "  optimized for large datasets.\n",
    "\n",
    "  Args:\n",
    "      X: A numpy array of complex numbers (shape: n_samples, features).\n",
    "      labels: A numpy array of cluster labels (shape: n_samples).\n",
    "\n",
    "  Returns:\n",
    "      The adjusted Silhouette score.\n",
    "  \"\"\"\n",
    "  n_samples = X.shape[0]\n",
    "  silhouette_values = np.zeros(n_samples)\n",
    "\n",
    "  # Vectorize distance calculation for efficiency\n",
    "  distances = np.abs(np.real(X)[:, np.newaxis, :] - np.real(X)[np.newaxis, :, :]) + \\\n",
    "              np.abs(np.imag(X)[:, np.newaxis, :] - np.imag(X)[np.newaxis, :, :])\n",
    "\n",
    "  # Calculate average distance within cluster (a_i) using efficient indexing\n",
    "  cluster_indices = np.arange(n_samples)[:, np.newaxis] == cluster_indices[np.newaxis, :]\n",
    "  a_i = np.mean(distances[cluster_indices & ~np.eye(n_samples, dtype=bool)], axis=1)\n",
    "\n",
    "  # Find minimum distance to other clusters (b_i)\n",
    "  b_i = np.min(distances[~cluster_indices], axis=1)\n",
    "\n",
    "  # Calculate silhouette score for all samples\n",
    "  silhouette_values = (b_i - a_i) / np.maximum(a_i, b_i)\n",
    "\n",
    "  return np.mean(silhouette_values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'load_metrics_function' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Input \u001b[1;32mIn [4]\u001b[0m, in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;66;03m# bert_train_dataset.Dim_reduction('zca').shape\u001b[39;00m\n\u001b[0;32m      2\u001b[0m \n\u001b[0;32m      3\u001b[0m \u001b[38;5;66;03m# load_metrics_function('silhouette_score', [np.random.rand(96000, 256), bert_train_dataset.data['target']])\u001b[39;00m\n\u001b[1;32m----> 4\u001b[0m \u001b[43mload_metrics_function\u001b[49m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mdavies_bouldin_score\u001b[39m\u001b[38;5;124m'\u001b[39m, [np\u001b[38;5;241m.\u001b[39marray(bert_train_dataset\u001b[38;5;241m.\u001b[39mDim_reduction(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124msphere\u001b[39m\u001b[38;5;124m'\u001b[39m)), bert_train_dataset\u001b[38;5;241m.\u001b[39mdata[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtarget\u001b[39m\u001b[38;5;124m'\u001b[39m]])\n",
      "\u001b[1;31mNameError\u001b[0m: name 'load_metrics_function' is not defined"
     ]
    }
   ],
   "source": [
    "# bert_train_dataset.Dim_reduction('zca').shape\n",
    "\n",
    "# load_metrics_function('silhouette_score', [np.random.rand(96000, 256), bert_train_dataset.data['target']])\n",
    "load_metrics_function('davies_bouldin_score', [np.array(bert_train_dataset.Dim_reduction('sphere')), bert_train_dataset.data['target']])\n",
    "# adjusted_silhouette_score(np.array(bert_train_dataset.Dim_reduction('zca')), bert_train_dataset.data['target'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BERT -> silhouette_score -> whitening\n",
      "BERT -> silhouette_score -> pca\n",
      "BERT -> silhouette_score -> sphere\n",
      "BERT -> davies_bouldin_score -> whitening\n",
      "BERT -> davies_bouldin_score -> pca\n",
      "BERT -> davies_bouldin_score -> sphere\n",
      "BERT -> MSE -> whitening\n",
      "BERT -> MSE -> pca\n",
      "BERT -> MSE -> sphere\n",
      "BERT -> RMSE -> whitening\n",
      "BERT -> RMSE -> pca\n",
      "BERT -> RMSE -> sphere\n",
      "ROBERTA -> silhouette_score -> whitening\n",
      "ROBERTA -> silhouette_score -> pca\n",
      "ROBERTA -> silhouette_score -> sphere\n",
      "ROBERTA -> davies_bouldin_score -> whitening\n",
      "ROBERTA -> davies_bouldin_score -> pca\n",
      "ROBERTA -> davies_bouldin_score -> sphere\n",
      "ROBERTA -> MSE -> whitening\n",
      "ROBERTA -> MSE -> pca\n",
      "ROBERTA -> MSE -> sphere\n",
      "ROBERTA -> RMSE -> whitening\n",
      "ROBERTA -> RMSE -> pca\n",
      "ROBERTA -> RMSE -> sphere\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'BERT': {'silhouette_score': [0.004910440434974939,\n",
       "   0.061955944,\n",
       "   0.053963706374487004],\n",
       "  'davies_bouldin_score': [13.718685090073835,\n",
       "   3.8970034313580086,\n",
       "   4.00669547576673],\n",
       "  'MSE': [0.0013618813768084324, 0.0013498877, 0.0013582603230595713],\n",
       "  'RMSE': [0.0013618813768084324, 0.0013498877, 0.0013582603230595713]},\n",
       " 'ROBERTA': {'silhouette_score': [0.004676833701088986,\n",
       "   0.037115417,\n",
       "   0.03039712149613265],\n",
       "  'davies_bouldin_score': [14.157277478047295,\n",
       "   4.646275547919451,\n",
       "   5.024773399950326],\n",
       "  'MSE': [0.002129265310768291, 0.0021421236, 0.002113015893207719],\n",
       "  'RMSE': [0.002129265310768291, 0.0021421236, 0.002113015893207719]}}"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "for i, architecture in enumerate(architectures):\n",
    "    dataset = bert_train_dataset if architecture == 'BERT' else roberta_train_dataset\n",
    "    for j, metric in enumerate(metrics):\n",
    "        for k, method in enumerate(methods):\n",
    "            print(f\"{architecture} -> {metric} -> {method}\")\n",
    "            labels = dataset.data['target']\n",
    "            vecs_before = np.array(dataset.embedding)\n",
    "            vecs_after = dataset.Dim_reduction(method)\n",
    "\n",
    "            if(metric in [\"silhouette_score\", \"davies_bouldin_score\"]):\n",
    "                data[architecture][metric].append(load_metrics_function(metric, [vecs_after, labels]))\n",
    "\n",
    "            if(metric in [\"MSE\", \"RMSE\"]):\n",
    "                data[architecture][metric].append(load_metrics_function(metric, [np.mean(vecs_before, axis=1), np.mean(vecs_after, axis=1)]))\n",
    "\n",
    "\n",
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import silhouette_score, davies_bouldin_score, mean_squared_error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# higher better\n",
    "print(silhouette_score(np.array(train_dataset.vecs_before), train_dataset.data['target']))\n",
    "print(silhouette_score(np.array(train_dataset.vecs_after), train_dataset.data['target']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# lower better\n",
    "print(davies_bouldin_score(np.array(train_dataset.vecs_before), train_dataset.data['target']))\n",
    "print(davies_bouldin_score(np.array(train_dataset.vecs_after), train_dataset.data['target']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(mean_squared_error(np.mean(np.array(train_dataset.vecs_before), axis=1), np.mean(np.array(train_dataset.vecs_after), axis=1)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAA1gAAAGoCAYAAABbkkSYAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjUuMSwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy/YYfK9AAAACXBIWXMAAAsTAAALEwEAmpwYAABEV0lEQVR4nO3de5yVdbn//9cVoIAgoFgiqENkhoWNqSUmhZWHUrG29VUr0W17u/1uxexktrOvVLa3pqXbUwd/FR4SSd2aW1PzhEqYpjkYZOQJBDwhHhAEheHz++O+Z1qMA7OYudestWZez8djPWat+7Sue839njXXfd/rXpFSQpIkSZLUdW+rdgGSJEmS1FPYYEmSJElSQWywJEmSJKkgNliSJEmSVBAbLEmSJEkqiA2WJEmSJBXEBkuSJEmSCmKDVYMi4j8i4v/L7zdERIqIvvnjmRHxL9WtUOp+ETEtIs7o4jJ+GhHfKaqmNsueGhFXVGjZKSLetYFxx0TErJLHKyLinZWoQ6oVEbEgIt6MiOFthj+c56UhIkZFxLUR8WJEvBoRcyPimHy6lvfWFW1uh1dlhaRulmdoVb7dP5e/xw7Kx03L83Fom3nOzYcfkz/eLCJ+FBGL8+UsiIjzNvAcLbcLu3M9q6VvtQvQW6WU/rPaNbQVEQuAf0kp3Z4/bgCeAvqllNZWsTSpbCml46tdQ6WllAZVuwapmzwFHAlcABAR44CBJeMvB+YAOwJvAOOAbdssY6jvYerFDkkp3R4R2wK3At8Cvp2P+zswGfgtQL6j//8AT5TM/y1gD+CDwLNkWftIe89RsTWoUR7BUo/WcuRPUm0wkyrQ5WT/ALY4Gris5PGewLSU0sqU0tqU0sMppZu7tUKpDqSUniNrsBpLBv8vsE9EDMsfHwg8AjxXMs2ewHUppWdSZkFKqTSDvZYNVpVFxDcjYklEvBYR8yPi42WcarRjRPwhn+f3padIRMSkiJgXEa/kpxOOLRm33mlGbU+5ioiDI6Ipn3d2ROyaD78c2AH43/zw7inAPflsr+TDxufTHhsRj0bEyxFxa0Ts2MH6R37I+YWIWB4Rf4mI9+XjBuSHnhfmp3fMiogBZazngvx1fQRYGRF9I2KvfJ1eiYg5ETFx478ZVVtE7BYRf8638xlA/3z4sIi4MSKW5tvZjRExKh93eEQ82GY5X4mIG/L7ZW3z+bi3ZLOMsvtHxIx8nj9HxPtLljc231ZfybfdSSXj1jv1N9qc9tdmfbaOiBvyvDwAjGkzvjXn+fpeFBE35TXdHxFj2ltuyfxmUvXij8CWebb6AEcAV7QZf1FEHBERO1SlQqkO5O+hnwQeLxm8muzo1RH548msvwMDsox9NSL+PSLGRURUvNg6YYNVRRGxM3AisGdKaTBwALCgjFk/D/wz8HZgM+Dr+fLeDUwHTga2AX5H1hRtVkYtuwG/BP4N2Br4GXBDRGyeUjoKeJrsMO+glNIP+cch4KH5sPsiO1f3P4B/yp//3ryejdk/X9a7gSFkh5+X5ePOAXYH9ga2Ak4B1pW5nkcCBwFDgXcANwFn5Mv5OnBtRGzT0eui6sh/l9eT7aHeCrgaOCwf/TbgV2SnIuwArAJazun+X2DniNipZHGfB65s5zk2uM13IZuH5rVulT/n9RHRLyL65bX9niy3U4Bf58+zqS4ie+MbARyb3zbmCOC7wDCyN88fdDC9mVQ9aTmKtR/wKLCkZNznyN6HvgM8le9M2bPN/C/mTX7LbSxS73F9RLwGLAJeAE5vM/4yYHJEDAU+Sva+XOq/gLOALwAPAksi4uh2nqM0Y/9a8DrUJBus6moGNgd2iYh++aHVJzqaCfhVSunvKaVVwG/4xyHdw4GbUkq3pZTWkP0zNIDsn6GOHAf8LKV0f0qpOaV0Kdk563ttwvocD/xXSunR/Jz2/wQaY+NHsdYAg4H3AJHP+2xEvI3sH8cvp5SW5DXNTim9UeZ6np9SWpS/Rl8EfpdS+l1KaV1K6TayPwSf2oR1U/faC+gHnJdSWpNSugb4E0BKaVlK6dqU0usppdfIGoaP5uNeJ9vjdiRA3mi9B7ihnefY2Dbf2Ww+lFK6Jt8uf0x21G2v/DYIODOl9GZK6U7gxpY6y5XvpT8M+H/5aU9zgUs7mO26lNIDeSZ/zfqngLTHTKqeXE62E+UY2uxdTym9nFI6NaX0XrKmvonsn73SvezDU0pDS26PdlPdUi34dL4TcSLZ3/z1LhqTUppFttPs28CN+d/v0vHNKaWLUkofJtt59gPgl212VHy6TcYuqdzq1A4brCpKKT1Otsd3KvBCRFwVEduVMWvp+a+vk/3jBrAdsLBk+evI9kqMLGOZOwJfK93LAGyfL7NcOwL/XTL/S0Bs7PnzfzQvJNsr/0JE/DwitiQLeX/W/zBli3LWc1Gbuj7XZt32ITsCoNq0HbAkpZRKhi0EiIiBEfGz/DS15WSnqw7Nmw/Ijhy1NC6fB67PG6+2NrjNdyGbrdtdvl0uztdlO2BRPqx0fcrJZqltyC5OVLp9L9zAtC029PeiXWZS9SSltJDsYhefAv5nI9O9SNb4b0d21FRSLqV0NzCNLCNtXQF8jbeeHth2GatSShcBLwO7FF1jvbHBqrKU0pUppX3I/uFIZIdaO+uZfDlA9lkKsn8YW06ZeJ31r7BUejWlRcAP2uxlGJhSajnFr/Qf3fYetyzj39osY0BKafbGik4pnZ9S2p0skO8GvgG8SHYaVHufF+loPdvWtwi4vE1dW6SUztxYXaqqZ4GRbfY0t3yG4mvAzsCHUkpb8o/TVVumvQ3YJiIayRqtt5wemNvoNt/JbG7fcic/4jOKbHt9Btg+H1a6Pi3b7Eo2nM1SS4G1pc/DP16XwphJ1ZkvAR9LKa0sHRgRZ0XE+/LP/A0G/i/weEppWbtLkXq384D9ouSzw7nzyU7BvaftDBFxckRMzD+f2zc/PXAw8HDFq61xNlhVFBE7R8THImJzsn9cVgHrOphtY34DHBTZhTL6kf0j+gbQ0uA0AZ+PiD4RcSD5aVW5S4DjI+JDkdkiIg7K35QAngdKv1tnaV5r6bCfAt+KiPfm6zckIj63sYIjYs/8OfuR/ZO5GliX7wH/JfDjiNgur3l8/lp1tJ5tXQEcEhEH5Mvpn/9BGLWx2lRV95E1Eifln2H6J7LLwEL2x3sV2QVWtqLNOeP5KWpXA2eT7am+bQPPscFtvgvZ3D0i/imyK+WdTLZd/hG4n2wHxyn5+kwEDgGuyudrAv4pPzr3LrJ/GN8ipdRMtpd+aj7tLmRXTiuMmVS9SSk9kVJ6sJ1RA4HrgFeAJ8l2AkxqM03LhZpabl+tbLVSbUopLSU7SvX/2gx/KaV0R5szSlq8DvyI7EyJF4ETgMNSSk+WTPO/bTJ2XYVWobaklLxV6QbsCjwAvEZ2Ot2NZKcvTAWuyKdpINvz2zd/PJPs+6halnEMMKvk8WeAvwKvAncD7y0ZtwcwL3++y8k+lH5GyfgDyT7n8grZEYSrgcH5uEPJLnTxCvD1fNj3yBqtV4C98mFHAX8BlpPtpf5lB6/Bx8ku+7mCLJy/Bgbl4waQ7VFZkq/PPcCAMtZzAfCJNs/zoXy6l/KabwJ2qPY24G2j28YeZHvBXgNm5Lcz8ozMzLeZv5NdpKI1I/m8E/JhF7VZ5rRytvkNZbODeqcC1+R1vpbX/oGS8e/Nt8FX8233MyXjhpNdAOM14A/5skpznYB35fe3yetZntf4/Y1M23Z9JwKLO1gPM+nNmzdv3rx14RYptdeQSpIkSZI2lacISpIkSVJBbLBUcRExoc35t623atcmbYqIuHkD2/J/VLu2TWEmJUmqHE8RlCRJkqSCeASrSiLivyLi5E2YfkJEzC9z2okRsbjTxXW8/C9ExO87Md/MiPiXStTUWRHxQMtVD9UzbGq2qiki/iMi/r9OzLcgIj5RiZo6IyI2j4i/RcQ21a5FxTNT3c9M9Xzmqvt1Z65ssKog/8VOBn6WP263ISptSFJK96aUdu7eStuXUvp1Smn/7nzO9pqziEj5Ja274hyyqyGqByg3W7UipfSfKaVu3enQ9g0vIhryLPXt7DJTSm+QXcL91CJqVO0wUx0zU9pU5qpj9Z4rG6zqOAb4XUppVbUL6UhXNuRaVrJeNwD7RsSGvthV9eUYajRbvSBLVwJH59+LpZ7jGMxUtzJTvcIxmKtu1d25ssGqjk+Sff9L2dru3YiID0TEwxHxWkRcHREzIuKMNvN8LSJeiIhnI+KfS4ZvHhHnRMTTEfF8RPw0IgaUPk9EfDMingN+1U4tx0TErPx+RMS5+fMsj4i/RMT7NrIqY/LT8pZHxG8j+6LYluXuFRGzI+KViJgT2ZexEhE/IPteowsj+yD+hRHR8o3ic/Jhh+fTHhwRTfkyZkfEriXLX5Cv1yPAyojom1JaDTwEHFDO70E1r+xsRcTY/MjoKxExLyIm5cNH58Pelj++JCJeKJnv8pbTOiL7Mu1f5BlbEhFnRESffNwxEfGHPB/LyL7bqm0NUyPiivx+/4i4IiKW5c//p4h4x0ZWYc+I+GtEvBwRv4qI/iXLbTcHEXE5sAP/+OLHU8i+ywr+8YWr4/Npj42IR/Pl3xoRO5YsP0XECRHxGPAYQEppMfAysFc5r7/qhpnCTKlw5ooenqtqfxFXb7yRfanmniWPJ9LOl39S8qXCpdMAmwELgS8D/YB/At4k/0LRfNq1ZKe+9QM+RfZt28Py8eeSHbnZiuxLVf8X+K82854FbE7+JaJt6jqG/ItNyRqTh4ChQABjgREbWO+ZZF9Q+j5gC+Ba/vGFyiOBZXmtbwP2yx9v0/a1KFle6xeq5o93A14g+wLTPsDRZF9wunk+fgHQBGxful7A+cCPq71deOvWbPUDHgf+I8/Tx8i+5HfnfPzTwO75/fnAk8DYknG75fevIzvFYwvg7WRf/Ptv+bhj8ixNAfpuIEtTSzLwb3kWB+bb7+7AlhtYzwXA3Hxb3orsy4lb8l9ODj5RsqwG3vpFzYfmr8/YvPbTgNkl4xNwW/7cpVm6ATip2tuBt+JuZspMeSv+Zq56fq48glUdQ8kCUmq7vINvvQH7bGD+vcg2pPNTSmtSSv9DFpZSa4Dv5eN/B6wAdo6IAI4DvpJSeiml9Brwn8ARJfOuA05PKb2ROj58vYasSXsP2VUpH00pPbuR6S9PKc1NKa0EvgP8n3wvyhfJDpf/LqW0LqV0G/AgWcNVruOAn6WU7k8pNaeULgXeYP29FOenlBa1Wa/XyH4nqn9DeWu22rMXMAg4M6X0ZkrpTuBG4Mh8/N3AR+Mfp45ekz8eDWxJduT0HWTb58kppZUppRfIdl6UZumZlNIFKaW1ZWZpa7KdBs0ppYdSSss3Mv2F+bb8EvCDktrLyUFHjifb6fJoSmkt2d+IxtI9g/n4l8xSjzcUM2WmVLShmKsenaseeZ5lHXiZrCkp9UxKaVTpgIiYuYH5twOWpLwNzy1qM82yfGNr8TpZSLch2+vwUNZrZU9FtvegxdKUnTrXoZTSnRFxIXARsGNE/A/w9Y2ErbTOhWR7Z4YDOwKfi4hDSsb3A+4qp47cjmTn1U4pGbYZ2evV3vO3GAy8sgnPo9rVXrbasx2wKKW0rmTYQrIjqZC9aU0CFpOdljATOApYDdybUlqX/wHvBzxbkqW3sf421t72tiGXk+3luyoihgJXAN9OKa3ZwPRts9SynZeTg47sCPx3RPyoZFiQvT4L23n+Fmap5zFTZkrFM1c9PFcewaqOR4B3d2H+Z4GRUZIUso29HC8Cq4D3ppSG5rchKaVBJdNs0pejpZTOTyntDuxCtl7f2MjkpXXuQLYn5EWyAFxeUtPQlNIWKaUzN6GmRcAP2ixjYEppemm57cw3FphTxvJV+8rN1jPA9i3nrud2IDuFFbI3rQlkp23cDcwCPgx8lH+cN7+IbG/b8JLtbcuUUull/8vOUn60+bsppV2AvYGDya4ytSFts/RMSV0by0HbmtqrcRHZ6SOlyxiQUprdwXxmqecxU2ZKxTNXPTxXNljV8Tuyjb+z7gOagRMjom9EHAp8sJwZ870glwDnRsTbASJiZER06iIPEbFnRHwoIvoBK8n2mqzbyCxfjIhdImIg2WfErkkpNZPtATkkIg6IiD75hygnRkTLUb3ngXe2WVbbYZcAx+f1RERsEREHRcQG9xLlH7bcnewcXdW/drOVb0+tN7JTal8HTomIfpFdUOUQ4CqAlNJjZDsivgjcnR+RfR44jPxNK2Wnwv4e+FFEbBkRb4uIMRHRqWxHxL4RMS4/ZXY52c6HjWXphIgYFdmFYr4NzMiHd5SDtrlZmj9P6bCfAt+K/DviIvuA9Oc6qH8k2XnufyxrhVUvzJSZUvHMVQ/PlQ1WdVwGfCryK/dtqpTSm2QXtvgS2SHOL5Kdk/tGmYv4JtmHAv8YEcuB24HOfsfWlmQheZnscOwy4OyNTH85MA14DugPnASQUlpE9mHF/yAL0SKyI2Et2+h/A5+N7Cox5+fDpgKXRvaZtf+TUnoQ+Ffgwryex8k+vLkxhwAzU0rPdDCd6kN72RpJ9gZUetue7Hf/SbIjqBcDk1NKfyuZ726yU20XlTwO4M8l00wmO6Xhr2Tb3DXAiE7Wvm0+/3Lg0fz5Lt/I9FeSvWk+CTwBnAFQRg7+Czgtz83XU0qvk50X/4d82F4ppevILnRzVf43Yi7Za7UxnwcuTdn3jKjnMFNmSsUzVz08V7H+x3jUXSLiP4EXUkrnFbS8+4GfppR+VcTyeov8dftSSmlutWtRMYrOljoW2feJzAE+krIPUKsHMVPdz0z1fOaq+3Vnrmyw6lR+aHc+2R6NL5AdJn1n2vgV/CRJkiRVkFcRrF87A78h+06DJ4HP2lxJkiRJ1eURLEmSJEkqiBe5kCRJkqSCeIpgJw0fPjw1NDRUuwzViIceeujFlNI21a6jnpkplTJTXWemVMpMdZ2ZUlsbypUNVic1NDTw4IMPVrsM1YiIWNjxVNoYM6VSZqrrzJRKmamuM1Nqa0O58hRBSZIkSSqIDZYkSZIkFcQGS5IkSZIK4mewJKmT1qxZw+LFi1m9enW1S6lb/fv3Z9SoUfTr16/apagGmKmuM1Nqy1x13abmygZLkjpp8eLFDB48mIaGBiKi2uXUnZQSy5YtY/HixYwePbra5agGmKmuMVNqj7nqms7kylMEJamTVq9ezdZbb+0bVidFBFtvvbV7VdXKTHWNmVJ7zFXXdCZXNliS1AW+YXWNr5/acpvoGl8/tcftoms29fWzwZIkSZKkgvgZLEkqSMOpNxW6vAVnHtThNH369GHcuHGklOjTpw8XXnghe++9NwsWLGDs2LHsvPPOrdN+9atfZfLkyTQ0NDB48GAigmHDhnHZZZdx8skn89RTT7FixQqWLl3aep75xRdfzN57713oeknlMlNS8cxV5dlgSVIdGzBgAE1NTQDceuutfOtb3+Luu+8GYMyYMa3j2rrrrrsYPnw4p59+OmeccQbXXXcdADNnzuScc87hxhtv7I7ypZpjpqTi9bZceYqgJPUQy5cvZ9iwYZs0z/jx41myZEmFKpLqm5mSitcbcuURLEmqY6tWraKxsZHVq1fz7LPPcuedd7aOe+KJJ2hsbGx9fMEFFzBhwoT15r/lllv49Kc/3U3VSrXPTEnF6225ssGSpDpWetrFfffdx+TJk5k7dy6w8dMu9t13X1566SUGDRrE97///W6qVqp9ZkoqXm/LlacISlIPMX78eF588UWWLl3a4bR33XUXCxcupLGxkdNPP70bqpPqj5mSitcbcuURrM565mGYOqTaVXSvqa9WuwL1ZPWYqcP/WO0K1vO3v/2N5uZmtt56a15//fUOp+/bty/nnXce48aN47TTTmOrrbbqhirVbeoxUwf8Bp6p4JfkPvNwx9Okda3T/e3xp2he8yZbv/E0rz//PKxd3f4ymt+E5x6h75vDOO/ULzHu44dz2pcmsdWwIfDiY7D61fKeuyu2262yy1d9ZgrMVVd0Mlc2WJJUkHIuVVu0lvPaAVJKXHrppfTp0wd463ntxx57LCeddNJ6848YMYIjjzySiy66iO985zvdVbZUlgUnbdftz7lq9Rs07ncEkGfqvO/+I1MLF7eOAzj2iEM56UtHrjf/iHdsw5GfPoCLpv2G73zlX7uvcKlM5qrybLAkqY41Nze3O7yhoYFVq1a1O27BggXrPb7gggta70+cOJGJEycWVZ5Ud5oXPdju8Ibtt2PVE/e1O27B/et/r9AFZ3yz9f7Evfdg4t57FFegVId6W678DJYkSZIkFcQGS5IkSZIKYoMlSZIkSQWxwZIkSZKkgthgSZIkSVJBbLAkSZIkqSBepl2SilL0F1CW8eXeffr0Ydy4caxdu5bRo0dz+eWXM3ToUADmzZvHlClTWLJkCevWrWPy5MmcdtppRATTpk3jG9/4BiNHjmTNmjWMHTuWyy67jIEDBzJ16lQuueQSttlmm9bnmTlzJk1NTRx66KGMHj2a1atXc/DBB3P00Udz1FFHAfD0008zZMgQhgwZwvDhw7n99ttpampit9124+abb+bAAw8s9vVRz/fzicUu77iZHU7SZ/s9GPeed7G2uZnR22/H5eefwdAhgwGYN/8Jppx2FkueW5pl6rMHc9rJ/5JlasYNfOOM8xi57dtZs3YtY981msvO/x4DBwxg6o9+yiVXXsc2Ww1rfZ6Z11xC07z5HHrsVxm9/XasfuNNDv7EBI7+3MEcdVL2nXRPP/McQwYPYsjgQQzfaii3z/gpTXPns9sBR3LzFRdw4L4fLvb1Ue9griqeK49gSVIdGzBgAE1NTcydO5etttqKiy66CMi+gHjSpEmceuqpzJ8/nzlz5jB79mwuvvji1nkPP/xwmpqamDdvHpttthkzZsxoHfeVr3yFpqam1ltL0zZhwgSampp4+OGHufHGG1m+fHnrNJMmTeLss8+mqamJ22+/HYDp06ezzz77MH369O57UaQuGNB/c5puu4q5d17NVkOHcNG0LBerVq1m0j9/hVNP/Gfm33sdc26/itkPzeHiS3/TOu/hk/an6barmHfXNWy2WT9m3PD71nFf+dcv0HTbVa23ln8uJ3ywkabbruLhW6/kxtvvZfmKla3TTNrvo5x92sk03XYVt8/4KQDTf3sL+3ywkenX39qNr4rUNb0tVzZYktRDjB8/niVLlgBw5ZVX8uEPf5j9998fgIEDB3LhhRdy5plnvmW+tWvXsnLlSoYNG/aWcRsyYMAAGhsbW5+vPSklrr76aqZNm8Ztt93G6tWrN3GNpOoav/uuLHluKQBXXn8LH97j/ez/0fEADBwwgAvP+CZnXjjtLfOtXbuWla+vYtiQLct+rgED+tP43nez5NkXNjhNSomrb7ydaed+l9vu/SOrV7+xaSsk1YDekCsbLEnqAZqbm7njjjuYNGkSkJ0euPvuu683zZgxY1ixYgXLly8HYMaMGTQ2NjJy5EheeuklDjnkkNZpzz33XBobG2lsbGTfffd9y/O9/PLLPPbYY3zkIx/ZYE2zZ89m9OjRjBkzhokTJ3LTTTcVsapSt2hubuaOWQ8waf9sG583/wl233XsetOMadieFa+/zvLXVgAw44bf07jfEYzc/UBeeuVVDtnvH/k495Jf07jfETTudwT7fva4tzzfy68s57GnnuYje31ggzXNfnAOo7ffjjEN2zNx/B7cdMesIlZV6ja9JVc2WJJUx1atWkVjYyPbbrstzz//PPvtt1/Z87acIvjcc88xbtw4zj777NZxpacI3nXXXa3D7733Xt7//vczcuRIDjjgALbddtsNLn/69OkcccQRABxxxBGeJqi6sGr1GzTudwTbNu7H8y8uY7+P7FX2vC2nMj3XdBvj3rMTZ//kstZxpacy3XXNz1uH3/tAE+//xOGM3P1ADvjoeLZ9+/ANLn/69bdwxKEHAHDEoQcw/fpbOrGGUvfrbbmywZKkOtbyGayFCxeSUmr9DNYuu+zCQw89tN60Tz75JIMGDWLLLdc/vSIiOOSQQ7jnnns6fL4JEyYwZ84c5s2bxy9+8Quamprana65uZlrr72W733vezQ0NDBlyhRuueUWXnvttc6tqNRNWj4rsvCBm7JMTcs+C7LLu9/JQ488ut60Ty5czKCBA9ly8KD1hkcEh+z3Ee65/88dPt+EDzYy5/YZzLvran5x1W9pmju/3emam5u59nd38r1zL6HhQwcx5bSzuGXmbF5bsbKTayp1n96WKxssSeoBBg4cyPnnn8+PfvQj1q5dyxe+8AVmzZrVerGJVatWcdJJJ3HKKae0O/+sWbMYM2ZM2c83evRoTj31VM4666x2x99xxx3suuuuLFq0iAULFrBw4UIOO+wwrrvuuk1fOakKBg4YwPnfP4Uf/ezyLFOf+SSz/tTE7ffcD2Qfzj/pOz/klH8/ut35Zz3wMGN2HFX2843eYSSnnnAMZ108rd3xd8x6gF3HvotFD97MgvtvYuEDv+OwT32c626+q93ppVrUW3LlZdolqShlXFa9knbbbTd23XVXpk+fzlFHHcVvf/tbpkyZwgknnEBzczNHHXUUJ554Yuv0M2bMYNasWaxbt45Ro0Yxbdq01nHnnnsuV1xxRevj66+//i3Pd/zxx3POOeewYMECGhoa1hs3ffp0PvOZz6w37LDDDuMnP/kJkydPLmR91QuUcfnnStrtfe9h17E7Mf36Wzjqswfz21/+mCnf+SEnfPtMmtc1c9RhB3HiPx/eOv2MG37PrAeaWJfWMWrEO5h27ndbx517ya+54trftT6+/pc/fsvzHX/UZznnZ5ezYNEzNGy/3Xrjpl9/C5858GPrDTvsoI/zk8uuZvLnDi5qldUbmKtWlcpVpJQ6PXNvtsd2fdKDxw3qeMKepMr/PNayiHgopbRHteuoZ/WYqUcP/yNjx47teEJt1KOPPvqW19FMdV1dZuqA3zB2x7dXu4z6tN1urXfNVGXUY6bAXHVJJ3PlKYKSJEmSVBAbLEmSJEkqiA2WJHWBp1l3ja+f1pfcJrrI109vZa66alNfPxssSeqk/v37s2zZMt+4OimlxLJly+jfv3+1S1GN6P/qkyxbudZMdZKZUnvMVdd0JldeRVCSOmnUqFEsXryYpUuXVruUutW/f39GjSr/krvq2Ub9+SwW802WDnknENUup768mn2XkJlSW+aqCzqZKxssSeqkfv36MXr06GqXIfUY/d58hdF//Fa1y6hPXulXG2CuuqCTufIUQUmSJEkqiA2WJEmSJBXEBkuSJEmSCmKDJUmSJEkFscGSJEmSpILYYEmSJElSQWywJEmSJKkgNliSJEmSVBAbLEmSJEkqiA2WJEmSJBXEBkuSJEmSCmKDJUmSJEkFscGSJEmSpILYYEmSJElSQeq+wYqIaRHx2WrXIfUUZkoqnrmSimWmVMvqvsHqisj06tdAKpKZkopnrqRimSlVWk1uXBGxRUTcFBFzImJuRBweEQsi4ocR8ZeIeCAi3lUyy0ciYnZEPFm6NyMivhERf4qIRyLiu/mwhoiYHxGXAXOB7dubTupJzJRUPHMlFctMqafoW+0CNuBA4JmU0kEAETEEOAt4NaU0LiImA+cBB+fTjwD2Ad4D3ABcExH7AzsBHwQCuCEiPgI8nQ8/OqX0xw1Nl1K6p21REXEccBxAny23oWH1ryqy8jXr1JuqXUGXLDjzoGqXUE1mahP08m1F5au5XNVqpuqRfweqwkz1YL0pUzV5BAv4C7BfRJwVERNSSq/mw6eX/BxfMv31KaV1KaW/Au/Ih+2f3x4G/kwWvp3ycQtTSn8sY7r1pJR+nlLaI6W0R5+BQ7q8klI3MlNS8WouV2ZKdc5MqUeoySNYKaW/R8QHgE8BZ0TEHS2jSicruf9Gyf0o+flfKaWflS47IhqAlW2mf8t0Uk9ipqTimSupWGZKPUVNHsGKiO2A11NKVwBnAx/IRx1e8vO+DhZzK3BsRAzKlzkyIt7ehemkumWmpOKZK6lYZko9RU0ewQLGAWdHxDpgDfB/gWuAYRHxCNkeiyM3toCU0u8jYixwX0QArAC+CDSXOd0Lha6RVF1mSiqeuZKKZabUI0RKqeOpakBELAD2SCm9WO1aADYfsVMacfR51S5Dm6CSH66MiIdSSntU7AkqwExtWG/6IG6tqsdMQW3lqpYyVY962t8BM9V1ZqprelqmYMO5qslTBCVJkiSpHtXqKYJvkVJqqHYNUk9ipqTimSupWGZK9cgjWJIkSZJUEBssSZIkSSqIDZYkSZIkFcQGS5IkSZIKYoMlSZIkSQWxwZIkSZKkgthgSZIkSVJBbLAkSZIkqSA2WJIkSZJUEBssSZIkSSqIDZYkSZIkFcQGS1LNePmuX/Lm0gWseuphnvnVFABevOnHNK98pbqFSZIklalvtQuQpBabj3wPbyx5lOYVL9Fn0Fase+N1mle+Qp8thla7NEmSpLJ4BEtSzdh85C68seRR1r7yHFvsMpHXH7/f5kqSJNUVGyxJNaPPFkNpXvEyRB/6j9qF5Q/8D5uPHFvtsiRJkspmgyWppvQZNIzN3t5A3yHvYN3rr9pgSZKkuuJnsCTVlOEHfbX1/qgTLqtiJZIkSZvOI1iSJEmSVBAbLEmSJEkqiA2WJEmSJBXEBkuSJEmSCmKDJUmSJEkFscGSJEmSpILYYEmSJElSQWywJEmSJKkgNliSJEmSVBAbLEmSJEkqiA2WJEmSJBXEBkuSJEmSCmKDJUmSJEkFscGSJEmSpILYYEmSJElSQWywJEmSJKkgfatdQL0aN3IID555ULXLkHoMMyUVy0xJxTJTKpdHsCRJkiSpIDZYkiRJklQQGyxJkiRJKogNliRJkiQVxAZLkiRJkgpigyVJkiRJBbHBkiRJkqSC2GBJkiRJUkFssCRJkiSpIDZYkiRJklQQGyxJkiRJKogNliRJkiQVxAZLkiRJkgpigyVJkiRJBbHBkiRJkqSC2GBJkiRJUkFssCRJkiSpIDZYkiRJklQQGyxJkiRJKogNliRJkiQVxAZLkiRJkgpigyVJkiRJBbHBkiRJkqSC2GBJkiRJUkFssCRJkiSpIDZYkiRJklQQGyxJkiRJKogNliRJkiQVxAZLkiRJkgpigyVJkiRJBbHBkiRJkqSC2GBJkiRJUkFssCRJkiSpIDZYkiRJklQQGyxJkiRJKkjfahdQt555GKYOqXYV9WHqq9WuQPWgHjLltqx6Ug+ZUvH8O1U5Zqp36kSmPIIlSZIkSQWxwZIkSZKkgthgSZIkSVJBbLAkSZIkqSA2WJIkSZJUEBssSZIkSSqIDZYkSZIkFcQGS5IkSZIKYoMlSZIkSQWxwZIkSZKkgthgSZIkSVJBbLAkSZIkqSA2WJIkSZJUEBssSZIkSSqIDZYkSZIkFcQGS5IkSZIKYoMlqWatS6naJUiSJG2SvtUuQJJKzVywlh/f9yYA/3ePfnxyp35VrkiSJKl8NliSas6bzYlbvrhFtcuQJEnaZJ4iKKnmfGBEn2qXIEmS1Ck2WJJqztui2hVIkiR1ziY3WBHxtojYshLFSL2VuZKKZaakYpkpqXxlNVgRcWVEbBkRWwBzgb9GxDcqW5rUs5mr9k1s6MsZH+tf7TJUh8yUVCwzJXVOuUewdkkpLQc+DdwMjAaOqlRRUi9hrqRimSmpWGZK6oRyG6x+EdGPLGA3pJTWAH5BjdQ15koqlpmSimWmpE4ot8H6GbAA2AK4JyJ2BJZXqiiplzBXUrHMlFQsMyV1Qlnfg5VSOh84v2TQwojYtzIlSb2DuZKKZaakYpkpqXM22mBFxFc7mP/HBdYi9QrmSiqWmZKKZaakrunoCNbg/OfOwJ7ADfnjQ4AHKlWU1MOZK6lYZkoqlpmSumCjDVZK6bsAEXEP8IGU0mv546nATRWvTuqBzJVULDMlFctMSV1T7kUu3gG8WfL4zXyYpM4zV1KxzJRULDMldUJZF7kALgMeiIjrgAAOBaZVqiiplzBXUrHMlFQsMyV1QrlXEfxBRNwMTCD7/oN/Tik9XNHKpB7OXEnFMlNSscyU1DnlHsECaAbWkQVsXWXKkXodcyUVy0xJxTJT0iYq6zNYEfFl4NfAcODtwBURMaWShUk9nbmSimWmpGKZKalzyj2C9SXgQymllQARcRZwH3BBpQqTegFzJRXLTEnFMlNSJ5R7FcEgO0TcojkfJqnzzJVULDMlFctMSZ1Q7hGsXwH351eRAfg08IuKVCT1HuZKKpaZkoplpqROKPcqgj+OiLuBD+eDav4qMhHRANwCPAR8AJgHTAbeC/w3sAXwBvBxYGvg8nwYwIkppdndXLJ6mXrLlZlSrTNTUrHMlNQ5m3IVwSbg2ZZ5ImKHlNLTlSiqQDsDX0op/SEifgmcCBwPHJ5S+lNEbAmsAl4A9ksprY6InYDpwB5Vq1q9SRP1lSszpVrXhJmSitSEmZI2SVkNVn7FmNOB5/nH+bcJ2LVypRViUUrpD/n9K4BvA8+mlP4EkFJaDhARWwAXRkQj2fq9u72FRcRxwHEAfbbchobVv6ps9T3FqTdVu4JCLDjzoEKXV6e56hWZKvp3re5hpmo3U/XGvwEZM2WmitSbclXuEawvAzunlJZVspgKSG0eLwf6tzPdV8j+eLyf7MIfq9tdWEo/B34OsPmIndouW9pU9ZgrM6VaZqbMlIplpsyUOqHcqwguAl6tZCEVskNEjM/vfx74IzAiIvYEiIjBEdEXGEK2d2MdcBTQpyrVqrepx1yZKdUyMyUVy0xJnbDRI1gR8dX87pPAzIi4iezDgUD24ccK1laE+cAJ+Tm4fyX73oY7gQsiYgDZObifAC4Gro2IyWQfjlxZpXrVC9R5rsyUao6ZkoplpqSu6egUwcH5z6fz22b5Dd56CLYWrU0pfbHNsD8Be7UZ9hjrn0/8zYpWpd6unnNlplSLzJRULDMldcFGG6yU0ncBIuJzKaWrS8dFxOcqWZjUU5krqVhmSiqWmZK6ptzPYH2rzGE1I6W0IKX0vmrXIW1EXeXKTKkOmCmpWGZK6oSOPoP1SeBTwMiIOL9k1JbA2koWJvVU5koqlpmSimWmpK7p6DNYzwAPApPIvhW7xWtkl7eUtOnMlVQsMyUVy0xJXdDRZ7DmAHMi4sp82h1SSvO7pTKphzJXUrHMlFQsMyV1TbmfwToQaCK7jCUR0RgRN1SqKKmXMFdSscyUVCwzJXVCuQ3WVOCDwCsAKaUmYHRFKpJ6j6mYK6lIUzFTUpGmYqakTVZug7UmpdT2m7xr/XsQpFpnrqRimSmpWGZK6oSOLnLRYl5EfB7oExE7AScBsytXltQrmCupWGZKKpaZkjqh3CNYU4D3Am8A04HlwMkVqknqLcyVVCwzJRXLTEmdUNYRrJTS68C385ukApgrqVhmSiqWmZI6p6MvGt7olWJSSpOKLUfq+cyVVCwzJRXLTEld09ERrPHAIrLDwvcDUfGKpJ7PXEnFMlNSscyU1AUdNVjbAvsBRwKfB24CpqeU5lW6MKkHM1cbkNY18+KNP6L5tWX0Gbw1ww/+GvG2PtUuS7XPTEnFMlNSF2z0IhcppeaU0i0ppaOBvYDHgZkRcWK3VCf1QOZqw17/+33023p7tv3CWfQbvgOvz/9DtUtSHTBTUrHMlNQ1HV7kIiI2Bw4i24vRAJwPXFfZsqSezVy1b+0rz7LZtu8CYPNtd+LN5x6vckWqF2ZKKpaZkjqvo4tcXAa8D/gd8N2U0txuqUrqwczVhvUdOoI3n3ucgWP25I3nHqPf0BHVLkl1wExJxTJTUtd09D1YXwR2Ar4MzI6I5fnttYhYXvnypB7JXG3AwJ32Ys2LT/Pcr7/JmqULGbjzh6tdkuqDmZKKZaakLtjoEayUUrlfRCypTOZqw6JPX7Y59JvVLkN1xkxJxTJTUtcYIEmSJEkqiA2WJEmSJBXEBkuSJEmSCmKDJUmSJEkFscGSJEmSpILYYEmSJElSQWywJEmSJKkgNliSJEmSVBAbLEmSJEkqiA2WJEmSJBXEBkuSJEmSCmKDJUmSJEkFscGSJEmSpILYYEmSJElSQWywJEmSJKkgNliSJEmSVBAbLEmSJEkqSN9qF1Cvxo0cwoNnHlTtMqQew0xJxTJTUrHMlMrlESxJkiRJKogNliRJkiQVxAZLkiRJkgpigyVJkiRJBbHBkiRJkqSC2GBJkiRJUkFssCRJkiSpIDZYkiRJklQQGyxJkiRJKogNliRJkiQVxAZLkiRJkgpigyVJkiRJBbHBkiRJkqSC2GBJkiRJUkFssCRJkiSpIDZYkiRJklQQGyxJkiRJKogNliRJkiQVxAZLkiRJkgpigyVJkiRJBbHBkiRJkqSC2GBJkiRJUkFssCRJkiSpIDZYkiRJklQQGyxJkiRJKogNliRJkiQVxAZLkiRJkgpigyVJkiRJBbHBkiRJkqSC2GBJkiRJUkFssCRJkiSpIDZYkiRJklQQGyxJkiRJKogNliRJkiQVpG+1C6hbzzwMU4dsfJqpr3ZPLVJPUE6m1PP4d7JyzFTvY54qy0z1Tp3IlUewJEmSJKkgNliSJEmSVBAbLEmSJEkqiA2WJEmSJBXEBkuSJEmSCmKDJUmSJEkFscGSJEmSpILYYEmSJElSQWywJEmSJKkgNliSJEmSVBAbLEmSJEkqiA2WJEmSJBXEBkuSJEmSCmKDJUmSJEkFscGSJEmSpILYYEmSJElSQWywJEmSJKkgNliSJEmSVJC+1S5AklqklPj3m1Yzf9k6BvQLrvjMAIYNiGqXJUmSVDaPYEmqGTf+fS07DHkbdx69BSfu2Y+fPvhmtUuSJEnaJB7BklQzHn1xHVfNW8OtT6xl7ToYP6pPtUuSJEnaJDZYkmrGzlu/jcm79uNre28OwJrmVOWKJEmSNo2nCEqqGZN27suCV9bxsUtX8rFLV3Lz42urXZIkSdIm8QiWpJoREVzwqQHVLkOSJKnTPIIlSZIkSQWxwZIkSZKkgthgSZIkSVJBbLAkSZIkqSA2WJIkSZJUEBssSZIkSSqIDZYkSZIkFcQGS5IkSZIKYoMlSZIkSQWxwZIkSZKkgthgSZIkSVJBbLAkSZIkqSA2WJIkSZJUkKo1WBGxYgPDj4+Iyfn9YyJiuy48xx4RcX5n55fqiZmSimWmpOKZK/UGfatdQFsppZ+WPDwGmAs808llPQg8WEBZUt0yU1KxzJRUPHOlnqRiR7Ai4hsRcVJ+/9yIuDO//7GI+HV+/wcRMSci/hgR78iHTY2Ir0fEZ4E9gF9HRFNEDIiI3SPi7oh4KCJujYgR+TwzI+KsiHggIv4eERPy4RMj4saS5f4yn/bJltrycd+JiPkRMSsipkfE1yv1ukidZaakYpkpqXjmSqrsEax7ga8B55MFZfOI6AdMAO4BPg/8MaX07Yj4IfCvwBktM6eUromIE4Gvp5QezOe9ADg0pbQ0Ig4HfgAc27IuKaUPRsSngNOBT7RT03uAfYHBwPyI+AnQCBwGvB/oB/wZeKi9FYqI44DjAPpsuQ0Nq3+18Vfg1JtYcOZBG59GKp+ZUt2rsb+JZkobVGPbaj3pUbkyU8XpTZmqZIP1ELB7RGwJvEG24e5BFrCTgDeBG0um3a+D5e0MvA+4LSIA+gDPloz/n5JlNWxgGTellN4A3oiIF4B3AB8GfptSWg2sjoj/3VABKaWfAz8H2HzETqmDeqWimSmpWGZKKl6PypWZUmdUrMFKKa2JiKfIzqOdDTxCtvfgXcCjwJqUUsuG2lxGLQHMSymN38D4N8pY1hsl98t5TqlmmCmpWGZKKp65kip/FcF7ga+THRK+FzgeeLgkWB15jexwLsB8YJuIGA8QEf0i4r0F1PgH4JCI6B8Rg4CDC1imVClmSiqWmZKKZ67Uq3VHgzUCuC+l9DywOh9WrmnATyOiieyQ8GeBsyJiDtAE7N3VAlNKfwJuINvDcjPwF+DVri5XqhAzJRXLTEnFM1fq1aL8nQk9V0QMSimtiIiBZHtbjksp/Xlj82w+Yqc04ujzOlx2b/pAX28WEQ+llPaodh21opKZUn0r92+imVqfmep+Pe3920y91abmykx1TU/LFGw4V56Dmvl5ROwC9Acu7ehNS1KHzJRULDMlFc9cqSJssICU0uerXYPUk5gpqVhmSiqeuVKlVPozWJIkSZLUa9hgSZIkSVJBbLAkSZIkqSA2WJIkSZJUEBssSZIkSSqIDZYkSZIkFcQGS5IkSZIKYoMlSZIkSQWxwZIkSZKkgthgSaopq59+hJfvubzaZUiSJHWKDZYkSZIkFaRvtQtQ91mzZg2LFy9m9erV1S6lbvXv359Ro0bRr1+/apciSZKkGmSD1YssXryYwYMH09DQQERUu5y6k1Ji2bJlLF68mNGjR1e7HEmSJNUgTxHsRVavXs3WW29tc9VJEcHWW2/tEUBJkiRtkA1WL2Nz1TW+fpIkSdoYTxGUVHNW/nUmbz7zNwCG7H04/XfYtcoVSZIklccGqxdrOPWmQpe34MyDOpymT58+jBs3jpQSffr04cILL2TvvfdmwYIFjB07lp133rl12q9+9atMnjyZhoYGBg8eTEQwbNgwLrvsMk4++WSeeuopVqxYwdKlS1s/E3XxxRez9957F7pe6l79d9iVUcf/otplSJIkdYoNlrrVgAEDaGpqAuDWW2/lW9/6FnfffTcAY8aMaR3X1l133cXw4cM5/fTTOeOMM7juuusAmDlzJueccw433nhjd5QvSZIkbZSfwVLVLF++nGHDhm3SPOPHj2fJkiUVqkiSJEnqGo9gqVutWrWKxsZGVq9ezbPPPsudd97ZOu6JJ56gsbGx9fEFF1zAhAkT1pv/lltu4dOf/nQ3VStJkiRtGhssdavSUwTvu+8+Jk+ezNy5c4GNnyK477778tJLLzFo0CC+//3vd1O1kiRJ0qbxFEFVzfjx43nxxRdZunRph9PeddddLFy4kMbGRk4//fRuqE6SJEnadDZYqpq//e1vNDc3s/XWW5c1fd++fTnvvPO47LLLeOmllypcnSRJkrTpPEWwFyvnsupFa/kMFkBKiUsvvZQ+ffoAb/0M1rHHHstJJ5203vwjRozgyCOP5KKLLuI73/lOd5UtSZIklcUGS92qubm53eENDQ2sWrWq3XELFixY7/EFF1zQen/ixIlMnDixqPIkSZKkLvEUQUmSJEkqiA2WJEmSJBXEBkuSJEmSCmKDJUmSJEkFscGSJEmSpILYYEmSJElSQbxMe282dUjBy3u1w0n69OnDuHHjWLt2LaNHj+byyy9n6NChAMybN48pU6awZMkS1q1bx+TJkznttNOICKZNm8Y3vvENRo4cyZo1axg7diyXXXYZAwcOZOrUqVxyySVss802rc8zc+ZMmpqaOPTQQxk9ejSrV6/m4IMP5uijj+aoo44C4Omnn2bIkCEMGTKE4cOHc/vtt9PU1MRuu+3GzTffzIEHHljs6yNJkqQezyNY6lYDBgygqamJuXPnstVWW3HRRRcB2RcQT5o0iVNPPZX58+czZ84cZs+ezcUXX9w67+GHH05TUxPz5s1js802Y8aMGa3jvvKVr9DU1NR6a2naJkyYQFNTEw8//DA33ngjy5cvb51m0qRJnH322TQ1NXH77bcDMH36dPbZZx+mT5/efS+KJEmSegyPYHXSuJFDePDMg6pdRl0bP348jzzyCABXXnklH/7wh9l///0BGDhwIBdeeCETJ07khBNOWG++tWvXsnLlSoYNG1b2cw0YMIDGxkaWLFmywWlSSlx99dXcdtttTJgwgdWrV9O/f/9OrJk6w0xJxTJTUrHMlMrlESxVRXNzM3fccQeTJk0CstMDd9999/WmGTNmDCtWrGD58uUAzJgxg8bGRkaOHMlLL73EIYcc0jrtueeeS2NjI42Njey7775veb6XX36Zxx57jI985CMbrGn27NmMHj2aMWPGMHHiRG666aYiVlWSJEm9iA2WutWqVatobGxk22235fnnn2e//fYre96WUwSfe+45xo0bx9lnn906rvQUwbvuuqt1+L333sv73/9+Ro4cyQEHHMC22267weVPnz6dI444AoAjjjjC0wQlSZK0yWyw1K1aPoO1cOFCUkqtn8HaZZddeOihh9ab9sknn2TQoEFsueWW6w2PCA455BDuueeeDp9vwoQJzJkzh3nz5vGLX/yCpqamdqdrbm7m2muv5Xvf+x4NDQ1MmTKFW265hddee61zKypJkqReyQZLVTFw4EDOP/98fvSjH7F27Vq+8IUvMGvWrNaLTaxatYqTTjqJU045pd35Z82axZgxY8p+vtGjR3Pqqady1llntTv+jjvuYNddd2XRokUsWLCAhQsXcthhh3Hddddt+spJkiSp1/IiF71ZGZdVr6TddtuNXXfdlenTp3PUUUfx29/+lilTpnDCCSfQ3NzMUUcdxYknntg6/YwZM5g1axbr1q1j1KhRTJs2rXXcueeeyxVXXNH6+Prrr3/L8x1//PGcc845LFiwgIaGhvXGTZ8+nc985jPrDTvssMP4yU9+wuTJkwtZX0mSJPV8kVKqdg11aY899kgPPvhgtcvYJI8++ihjx46tdhl1r73XMSIeSintUaWSeoR6zJQqx0x1nZlSKTPVdWZKbW0oV54iKEmSJEkFscGSJEmSpILYYPUynhLaNb5+kiRJ2hgbrF6kf//+LFu2zCahk1JKLFu2jP79+1e7FEmSJNUoryLYi4waNYrFixezdOnSapdSt/r378+oUaOqXYYkSZJqlA1WL9KvXz9Gjx5d7TIkSZKkHstTBCVJkiSpIDZYkiRJklQQGyxJkiRJKkh4RbnOiYjXgPnVrmMjhgMvVruIDtR6jZtS344ppW0qWUxPVweZaqvWt9/21FPNZqqL6jBTXVFP23ZXdXZdzVQXmakeqyvr2m6uvMhF581PKe1R7SI2JCIerOX6oPZrrPX6eqCazlRb9bh91GPN6pK6ylRX9KZtuzetaw0yUz1QJdbVUwQlSZIkqSA2WJIkSZJUEBuszvt5tQvoQK3XB7VfY63X19PU2+tdb/VCfdaszutNv2/XVd2hN732rmsXeJELSZIkSSqIR7AkSZIkqSA2WJIkSZJUEBusNiLiwIiYHxGPR8Sp7YzfPCJm5OPvj4iGknHfyofPj4gDaq3GiNgvIh6KiL/kPz9WS/WVjN8hIlZExNcrUV9Xa4yIXSPivoiYl7+W/StVZ2/Q0e+iFkXEgvx33xQRD1a7nvZExC8j4oWImFsybKuIuC0iHst/Dqtmjdo0lXh/qsX8VWg935KHWlD0ukbE9hFxV0T8NX+P+nI3rk7d6S2Zgt6Tq5rJVErJW34D+gBPAO8ENgPmALu0mebfgZ/m948AZuT3d8mn3xwYnS+nT43VuBuwXX7/fcCSWqqvZPw1wNXA12vw99wXeAR4f/5460r8nnvLrZzfRS3egAXA8GrX0UGNHwE+AMwtGfZD4NT8/qnAWdWu01vZv8/C359qMX+Veh9uLw/VvlXodzoC+EA+zWDg79X+ndbqrbdkqlLrmo+rqVzVUqY8grW+DwKPp5SeTCm9CVwFHNpmmkOBS/P71wAfj4jIh1+VUnojpfQU8Hi+vJqpMaX0cErpmXz4PGBARGxeK/UBRMSngafy+iqlKzXuDzySUpoDkFJallJqrmCtPV05vwt1QkrpHuClNoNLt+tLgU93Z03qkkq8P9Vi/iryPryBPFRb4euaUno2pfRngJTSa8CjwMhuWJd61FsyBb0nVzWTKRus9Y0EFpU8XsxbX8TWaVJKa4FXyY5ilDNvtWssdRjw55TSG7VSX0QMAr4JfLfgmgqrEXg3kCLi1oj4c0ScUuFae7ruyk3REvD7yE61Pa7axWyCd6SUns3vPwe8o5rFaJNU4v2pFvNXD+/DRanouuanPu0G3F9k0T1Ib8kU9J5c1Uym+m5a3eoJIuK9wFlkR2NqyVTg3JTSivyAVi3qC+wD7Am8DtwREQ+llO6oblnqZvuklJZExNuB2yLib/mevLqRUkoR4fd0SD1QvsPyWuDklNLyatcj1btNzZRHsNa3BNi+5PGofFi700REX2AIsKzMeatdIxExCrgOmJxSeqLG6vsQ8MOIWACcDPxHRJxYYzUuBu5JKb2YUnod+B3Z+cfqnO7KTaFSSkvyny+Q5akSpwNXwvMRMQIg//lCletR+Srx/lSL+auH9+GiVGRdI6If2T+Cv04p/U9FKu8ZekumoPfkqnYyVc0Po9XajezoxJNkH25r+XDce9tMcwLrfzjuN/n997L+h+OepDIXuehKjUPz6f+pFl/DNtNMpXIXuejKazgM+DMwMF/O7cBB1d526/VWzu+i1m7AFsDgkvuzgQOrXdcGam1g/YtcnM36F7n4YbVr9Fb277Lw96dazF8l34fb5qHatwr9TgO4DDiv2utX67fekqlKrWvJfDWTq1rKVNVfjFq7AZ8iu0LIE8C382HfAybl9/uTXeHuceAB4J0l8347n28+8MlaqxE4DVgJNJXc3l4r9bVZxlQq1GAV8Hv+ItlFOObiP6gV+V3U8o3s6kRz8tu8Wq0ZmA48C6whO/L6JbLzzO8AHiPbObBVtev0tkm/08Lfn2oxfxVaz7fkodrrWYl1JTuFPZFd7bYpv32q2utZq7fekqkKrmvN5apWMhX5zJIkSZKkLvIzWJIkSZJUEBssSZIkSSqIDZYkSZIkFcQGS5IkSZIKYoMlSZIkSQWxwVKriPh2RMyLiEcioikiPlTtmqR6Z66kYpkpqVhmqnh9q12AakNEjAcOBj6QUnojIoaTfUlbZ5fXN6W0trACpTpkrqRimSmpWGaqMjyCpRYjgBdTSm8ApJReTCk9ExF7RsTsiJgTEQ9ExOCI6B8Rv4qIv0TEwxGxL0BEHBMRN0TEncAdEbFFRPwyn+/hiDi0misoVYG5koplpqRimakK8AiWWvwe+H8R8XfgdmAGcF/+8/CU0p8iYktgFfBlIKWUxkXEe4DfR8S78+V8ANg1pfRSRPwncGdK6diIGAo8EBG3p5RWdvO6SdVirqRimSmpWGaqAjyCJQBSSiuA3YHjgKVkwfo34NmU0p/yaZbnh333Aa7Ih/0NWAi0BOy2lNJL+f39gVMjogmYCfQHduiO9ZFqgbmSimWmpGKZqcrwCJZapZSayYIwMyL+ApzQicWU7p0I4LCU0vwCypPqkrmSimWmpGKZqeJ5BEsARMTOEbFTyaBG4FFgRETsmU8zOCL6AvcCX8iHvZtsr0R7IboVmBIRkU+7W+XWQKo95koqlpmSimWmKsMjWGoxCLggP1d2LfA42eHiX+XDB5Cdf/sJ4GLgJ/lejrXAMfmVZ9ou8/vAecAjEfE24CmyK9VIvYW5koplpqRimakKiJRStWuQJEmSpB7BUwQlSZIkqSA2WJIkSZJUEBssSZIkSSqIDZYkSZIkFcQGS5IkSZIKYoMlSZIkSQWxwZIkSZKkgvz/9wKyHUkLj+YAAAAASUVORK5CYII=",
      "text/plain": [
       "<Figure size 864x432 with 4 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Indicate higher-better or lower-better for each metric (optional)\n",
    "metric_info = {\n",
    "    \"silhouette_score\": \"Higher is better\",\n",
    "    \"davies_bouldin_score\": \"Lower is better\",\n",
    "    \"MSE\": \"Lower is better\",\n",
    "    \"RMSE\": \"Lower is better\"\n",
    "}\n",
    "\n",
    "\n",
    "# Number of metrics\n",
    "num_metrics = len(metrics)\n",
    "\n",
    "# Create the figure and subplots\n",
    "fig, axes = plt.subplots(1, num_metrics, figsize=(12, 6))  # Adjust figsize as needed\n",
    "\n",
    "# Loop through metrics and create subplots\n",
    "for i, metric in enumerate(metrics):\n",
    "    ax = axes[i]\n",
    "\n",
    "    # Extract data for current metric\n",
    "    scores_arch1 = data[architectures[0]][metric]\n",
    "    scores_arch2 = data[architectures[1]][metric]\n",
    "\n",
    "    # Create horizontal bar plot\n",
    "    bar_width = 0.35\n",
    "    index = range(len(methods))\n",
    "    bar1 = ax.barh(index, scores_arch1, bar_width, label=architectures[0])\n",
    "    bar2 = ax.barh([p + bar_width for p in index], scores_arch2, bar_width, label=architectures[1])\n",
    "\n",
    "    # Set labels and title\n",
    "    ax.set_xlabel(\"Score\")\n",
    "    ax.set_ylabel(\"Methods\")\n",
    "    ax.set_title(metric)\n",
    "\n",
    "    # Set y-axis ticks and labels\n",
    "    ax.set_yticks([p + bar_width / 2 for p in index])\n",
    "    ax.set_yticklabels(methods)\n",
    "\n",
    "    # Determine y-axis limits for annotation placement\n",
    "    y_min, y_max = ax.get_ylim()\n",
    "\n",
    "    # Add legend\n",
    "    ax.legend()\n",
    "\n",
    "    # Annotate bars with metric information (if provided)\n",
    "    if metric_info:\n",
    "        for j, (rect, info) in enumerate(zip(bar1 + bar2, metric_info[metric])):\n",
    "            y_center = rect.get_y() + rect.get_height() / 2\n",
    "            x_center = 0.1  # Adjust x-position for better visibility\n",
    "            ax.annotate(info, (x_center, y_center), ha='left', va='center', fontsize=8)\n",
    "\n",
    "    # Subplot title with metric information\n",
    "    ax.set_title(f\"{metric}\\n({metric_info[metric]})\")  # Add metric info in subtitle\n",
    "\n",
    "\n",
    "# Adjust layout and show the plot\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Define data\n",
    "methods = [\"Method 1\", \"Method 2\", \"Method 3\"]\n",
    "metric1_scores = [0.8, 0.7, 0.6]\n",
    "metric2_scores = [0.9, 0.8, 0.7]\n",
    "\n",
    "# Indicate higher-better or lower-better for each metric\n",
    "metric_info = [\"Higher is better\", \"Lower is better\"]\n",
    "\n",
    "# Create separate figures for each metric\n",
    "fig1, ax1 = plt.subplots()\n",
    "fig2, ax2 = plt.subplots()\n",
    "\n",
    "# Create bars for each metric\n",
    "bar_width = 0.35\n",
    "index = range(len(methods))\n",
    "\n",
    "ax1.barh(index, metric1_scores, bar_width, label=metric1_scores[0])\n",
    "ax2.barh(index, metric2_scores, bar_width, label=metric2_scores[0])\n",
    "\n",
    "# Set labels and titles for each plot\n",
    "ax1.set_xlabel(\"Score\")\n",
    "ax1.set_ylabel(\"Methods\")\n",
    "ax1.set_title(f\"Comparison of {metric_info[0]} for Different Methods\")\n",
    "\n",
    "ax2.set_xlabel(\"Score\")\n",
    "ax2.set_ylabel(\"Methods\")\n",
    "ax2.set_title(f\"Comparison of {metric_info[1]} for Different Methods\")\n",
    "\n",
    "# Set y-axis ticks and labels for both plots\n",
    "for ax in [ax1, ax2]:\n",
    "    ax.set_yticks([p + bar_width / 2 for p in index])\n",
    "    ax.set_yticklabels(methods)\n",
    "\n",
    "# Add legends for both plots\n",
    "for ax in [ax1, ax2]:\n",
    "    ax.legend()\n",
    "\n",
    "# Adjust layout and show plots\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(96000, 768)\n",
      "(768, 768)\n",
      "(768, 768)\n",
      "0.0013113076470131866\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "arr = np.random.rand(96000, 768)\n",
    "\n",
    "vecs = np.concatenate([arr], axis=0)\n",
    "mu = vecs.mean(axis=0, keepdims=True)\n",
    "\n",
    "# Center the data\n",
    "centered_vecs = vecs - vecs.mean(axis=0, keepdims=True)\n",
    "\n",
    "# Apply PCA\n",
    "pca = PCA()\n",
    "pca.fit(centered_vecs)\n",
    "res_pca = pca.transform(centered_vecs)\n",
    "\n",
    "print(res_pca.shape)\n",
    "print(np.cov(vecs.T).shape)\n",
    "print(pca.components_.shape)\n",
    "print(mean_squared_error(np.cov(vecs.T), pca.components_))\n",
    "\n",
    "# Calculate eigenvalues and eigenvectors\n",
    "eigenvalues, eigenvectors = np.linalg.eig(pca.components_)\n",
    "\n",
    "# Apply ZCA whitening\n",
    "zca_matrix = eigenvectors * np.diag(1 / np.sqrt(eigenvalues + 1e-5)) @ eigenvectors.T\n",
    "# whitened_vectors = np.dot(centered_vecs, zca_matrix)\n",
    "\n",
    "# print(whitened_vectors.shape)\n",
    "\n",
    "\n",
    "\n",
    "# cross_corr_matrix = np.corrcoef(arr.T, whitened_vectors.T)[:arr.shape[1], arr.shape[1]:]\n",
    "# cross_cov_matrix = np.cov(arr.T, whitened_vectors.T)[:arr.shape[1], arr.shape[1]:]\n",
    "# abs_cross_cov_matrix = np.abs(cross_cov_matrix)\n",
    "\n",
    "# print(cross_corr_matrix)\n",
    "# print(abs_cross_cov_matrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(768, 768)\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "arr = np.random.rand(96000, 768)\n",
    "vecs = np.concatenate([arr], axis=0)\n",
    "mu = vecs.mean(axis=0, keepdims=True)\n",
    "cov = np.cov(vecs.T)\n",
    "u, s, vh = np.linalg.svd(cov)\n",
    "W = np.dot(u, np.diag(s**0.5))\n",
    "W = np.linalg.inv(W.T)\n",
    "\n",
    "print(W.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cross_correlations = []\n",
    "cross_covariances = []\n",
    "for i in range(arr.shape[1]):  # Number of original features\n",
    "    original_feature = arr[:, i]\n",
    "    whitened_feature = whitened_vectors[:, i]\n",
    "    cross_corr = np.correlate(original_feature, whitened_feature)\n",
    "    cross_correlations.append(cross_corr) \n",
    "    cross_cov = np.cov(original_feature, whitened_feature)[0, 1]  # Extract covariance value\n",
    "    cross_covariances.append(cross_cov)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(-0.1500645834741608+0.22703344591863212j)\n",
      "(-1.563189027739125e-06+2.3649563632830636e-06j)\n"
     ]
    }
   ],
   "source": [
    "print(np.array(cross_correlations).mean())\n",
    "print(np.array(cross_covariances).mean())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "# Define the model class\n",
    "class TransformerAttention(nn.Module):\n",
    "    def __init__(self, input_dim, hidden_dim, num_heads):\n",
    "        super(TransformerAttention, self).__init__()\n",
    "        self.num_heads = num_heads\n",
    "        \n",
    "        # Define the linear layers for Q, K, V\n",
    "        self.query_layer = nn.Linear(input_dim, hidden_dim)\n",
    "        self.key_layer = nn.Linear(input_dim, hidden_dim)\n",
    "        self.value_layer = nn.Linear(input_dim, hidden_dim)\n",
    "        \n",
    "        # Scaling factor for the dot product attention\n",
    "        self.scale_factor = (hidden_dim // num_heads) ** 0.5\n",
    "    \n",
    "    def forward(self, input):\n",
    "        # Generate Q, K, V embeddings from the input\n",
    "        Q = self.query_layer(input)  # Query embeddings\n",
    "        K = self.key_layer(input)    # Key embeddings\n",
    "        V = self.value_layer(input)  # Value embeddings\n",
    "\n",
    "        print(input.shape)\n",
    "        print(Q.shape)\n",
    "        print(K.shape)\n",
    "        print(V.shape)\n",
    "        print(\"==============================================================\")\n",
    "\n",
    "        \n",
    "        # Reshape the embeddings for multi-head attention\n",
    "        batch_size, seq_len, hidden_dim = Q.shape\n",
    "\n",
    "        print(Q.view(batch_size, seq_len, self.num_heads, hidden_dim // self.num_heads).shape)\n",
    "\n",
    "        print(\"==============================================================\")\n",
    "\n",
    "        Q = Q.view(batch_size, seq_len, self.num_heads, hidden_dim // self.num_heads).transpose(1, 2)\n",
    "        K = K.view(batch_size, seq_len, self.num_heads, hidden_dim // self.num_heads).transpose(1, 2)\n",
    "        V = V.view(batch_size, seq_len, self.num_heads, hidden_dim // self.num_heads).transpose(1, 2)\n",
    "\n",
    "        print(Q.shape)\n",
    "        print(K.shape)\n",
    "        print(V.shape)\n",
    "        print(\"==============================================================\")\n",
    "        \n",
    "        # Compute the dot product attention scores\n",
    "        attention_scores = torch.matmul(Q, K.transpose(-2, -1)) / self.scale_factor\n",
    "        print(attention_scores.shape)\n",
    "        print(\"==============================================================\")\n",
    "        \n",
    "        # Apply softmax to normalize the attention scores\n",
    "        attention_weights = nn.functional.softmax(attention_scores, dim=-1)\n",
    "        print(attention_weights.shape)\n",
    "        print(\"==============================================================\")\n",
    "        \n",
    "        # Compute the weighted sum of values\n",
    "        output = torch.matmul(attention_weights, V)\n",
    "        print(output.shape)\n",
    "        print(\"==============================================================\")\n",
    "        \n",
    "        print(output.transpose(1, 2).shape)\n",
    "        # Reshape the output back to the original shape\n",
    "        output = output.transpose(1, 2).reshape(batch_size, seq_len, hidden_dim)\n",
    "        print(\"==============================================================\")\n",
    "        \n",
    "        return output \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([32, 4, 32, 10])\n",
      "torch.Size([32, 4, 10, 10])\n"
     ]
    }
   ],
   "source": [
    "a = torch.randn(32, 4, 10, 32)\n",
    "b = a.transpose(-2,-1)\n",
    "print(b.shape)\n",
    "print((torch.matmul(a,b) / 3.2 ).shape)\n",
    "# print(input_data.transpose(1, 2).transpose(-2, -1).shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0.0121, 0.2266, 0.4699],\n",
      "        [0.5219, 0.9296, 0.1143]])\n",
      "tensor([[0.2618, 0.3244, 0.4138],\n",
      "        [0.3156, 0.4744, 0.2100]])\n"
     ]
    }
   ],
   "source": [
    "input_data = torch.rand(2,3)\n",
    "result = nn.functional.softmax(input_data, dim=-1)\n",
    "print(input_data)\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([32, 10, 64])\n",
      "torch.Size([32, 10, 128])\n",
      "torch.Size([32, 10, 128])\n",
      "torch.Size([32, 10, 128])\n",
      "==============================================================\n",
      "torch.Size([32, 10, 4, 32])\n",
      "==============================================================\n",
      "torch.Size([32, 4, 10, 32])\n",
      "torch.Size([32, 4, 10, 32])\n",
      "torch.Size([32, 4, 10, 32])\n",
      "==============================================================\n",
      "torch.Size([32, 4, 10, 10])\n",
      "==============================================================\n",
      "torch.Size([32, 4, 10, 10])\n",
      "==============================================================\n",
      "torch.Size([32, 4, 10, 32])\n",
      "==============================================================\n",
      "torch.Size([32, 10, 4, 32])\n",
      "==============================================================\n",
      "Output shape: torch.Size([32, 10, 128])\n"
     ]
    }
   ],
   "source": [
    "# Input data: batch_size x seq_len x input_dim\n",
    "input_data = torch.randn(32, 10, 64)  # Batch of 32 sequences, each with length 10 and embedding dimension 64\n",
    "\n",
    "# Define the attention layer\n",
    "attention_layer = TransformerAttention(input_dim=64, hidden_dim=128, num_heads=4)\n",
    "\n",
    "# Apply the attention layer\n",
    "output = attention_layer(input_data)\n",
    "\n",
    "print(\"Output shape:\", output.shape)  # Expected output shape: (32, 10, 128)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1e-06"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "w = 10\n",
    ".1e-5"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
